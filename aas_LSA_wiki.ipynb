{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 스파크 고급 분석 6장\n",
    "\n",
    "## 숨은 의미 분석으로 위키백과 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'proxyUser': 'hduser', 'driverMemory': '4000M', 'conf': {'spark.jars.packages': 'graphframes:graphframes:0.3.0-spark2.0-s_2.11', 'spark.master': 'local[2]', 'spark.jars': 'hdfs://localhost:54310/jars/ch06-lsa-2.0.0-jar-with-dependencies.jar', 'spark.sql.crossJoin.enabled': 'true'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"proxyUser\": \"hduser\",\n",
    "    \"driverMemory\": \"4000M\", \n",
    "    \"conf\": {\"spark.jars.packages\": \"graphframes:graphframes:0.3.0-spark2.0-s_2.11\",\n",
    "             \"spark.master\": \"local[2]\",\n",
    "             \"spark.jars\": \"hdfs://localhost:54310/jars/ch06-lsa-2.0.0-jar-with-dependencies.jar\",\n",
    "             \"spark.sql.crossJoin.enabled\": \"true\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import edu.umd.cloud9.collection.XMLInputFormat\n",
      "import edu.stanford.nlp.ling.CoreAnnotations.{LemmaAnnotation, SentencesAnnotation, TokensAnnotation}\n",
      "import edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}\n",
      "import edu.umd.cloud9.collection.wikipedia.WikipediaPage\n",
      "import edu.umd.cloud9.collection.wikipedia.language.EnglishWikipediaPage\n",
      "import java.util.Properties\n",
      "import org.apache.hadoop.conf.Configuration\n",
      "import org.apache.hadoop.io.{LongWritable, Text}\n",
      "import org.apache.spark.ml.feature.{CountVectorizer, IDF}\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
      "import scala.collection.JavaConverters._\n",
      "import scala.collection.mutable.ArrayBuffer\n",
      "import org.apache.spark.mllib.linalg.{Vectors, Vector=>MLLibVector, SingularValueDecomposition, Matrix, Matrices}\n",
      "import org.apache.spark.ml.linalg.{Vector=>MLVector}\n",
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
      "import breeze.linalg.{DenseMatrix=>BDenseMatrix, SparseVector=>BSparseVector}\n"
     ]
    }
   ],
   "source": [
    "import edu.umd.cloud9.collection.XMLInputFormat\n",
    "import edu.stanford.nlp.ling.CoreAnnotations.{LemmaAnnotation, SentencesAnnotation, TokensAnnotation}\n",
    "import edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}\n",
    "import edu.umd.cloud9.collection.wikipedia.WikipediaPage\n",
    "import edu.umd.cloud9.collection.wikipedia.language.EnglishWikipediaPage\n",
    "import java.util.Properties\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.io.{LongWritable, Text}\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, IDF}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
    "\n",
    "import scala.collection.JavaConverters._\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "\n",
    "import org.apache.spark.mllib.linalg.{Vectors, Vector => MLLibVector, SingularValueDecomposition, Matrix, Matrices}\n",
    "import org.apache.spark.ml.linalg.{Vector => MLVector}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "import breeze.linalg.{DenseMatrix => BDenseMatrix, SparseVector => BSparseVector}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base: String = hdfs://localhost:54310/wikipedia/\n"
     ]
    }
   ],
   "source": [
    "val base = \"hdfs://localhost:54310/wikipedia/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비 \n",
    "\n",
    "* XML parsing하여 page별  seq of topic word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: String = hdfs://localhost:54310/wikipedia/wikipedia.xml\n",
      "conf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\n",
      "kvs: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = hdfs://localhost:54310/wikipedia/wikipedia.xml NewHadoopRDD[0] at newAPIHadoopFile at <console>:44\n",
      "rawXml: org.apache.spark.sql.Dataset[String] = [value: string]\n"
     ]
    }
   ],
   "source": [
    "val path = base + \"wikipedia.xml\"\n",
    "\n",
    "@transient val conf = new Configuration()\n",
    "conf.set(XMLInputFormat.START_TAG_KEY, \"<page>\")\n",
    "conf.set(XMLInputFormat.END_TAG_KEY, \"</page>\")\n",
    "val kvs = spark.sparkContext.newAPIHadoopFile(path, classOf[XMLInputFormat], classOf[LongWritable], classOf[Text], conf)\n",
    "val rawXml = kvs.map(_._2.toString).toDS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikiXmlToPlainText: (pageXml: String)Option[(String, String)]\n",
      "docTexts: org.apache.spark.sql.Dataset[(String, String)] = [_1: string, _2: string]\n",
      "res8: (String, String) =\n",
      "(Energy,\"Energy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In physics, energy is the quantitative property that must be transferred to an object in order to perform work on, or to heat, the object. The second law of thermodynamics imposes limitations on the capacity of a system to transfer energy by performing work, since some of the system's energy might necessarily be   in the form of heat instead. See e.g.     Energy is a conserved quantity; the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The SI unit of energy is the joule, which is the energy transferred to an object by the work of moving it a distance of 1 metre against a force of 1 newton.Common forms of energy include the kinetic energy of a moving object, the potential energy st..."
     ]
    }
   ],
   "source": [
    "/**\n",
    "* Returns a (title, content) pair.\n",
    "*/\n",
    "def wikiXmlToPlainText(pageXml: String): Option[(String, String)] = {\n",
    "    val page = new EnglishWikipediaPage()\n",
    "\n",
    "    // Wikipedia has updated their dumps slightly since Cloud9 was written, so this hacky replacement is sometimes\n",
    "    // required to get parsing to work.\n",
    "    val hackedPageXml:String = pageXml.replaceFirst(\n",
    "        \"<text bytes=\\\"\\\\d+\\\" xml:space=\\\"preserve\\\">\", \"<text xml:space=\\\"preserve\\\">\")\n",
    "\n",
    "    WikipediaPage.readPage(page, hackedPageXml)\n",
    "    \n",
    "    if (page.isEmpty || !page.isArticle || page.isRedirect || page.isDisambiguation || page.getTitle.contains(\"(disambiguation)\")) {\n",
    "        None\n",
    "    } else {\n",
    "      Some((page.getTitle, page.getContent))\n",
    "    }\n",
    "}\n",
    "\n",
    "val docTexts = rawXml.filter( _ != null ).flatMap(wikiXmlToPlainText)\n",
    "docTexts.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 표제어 추출, 단어 빈도-역빈도(TF-IDF) 계산하기\n",
    "\n",
    "* stemmization 반영\n",
    "* stopword.txt로 불용어 처리\n",
    "* CountVectorizer로 TF\n",
    "* IDF 기능으로 TF-IDF 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [
     0,
     6,
     32
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createNLPPipeline: ()edu.stanford.nlp.pipeline.StanfordCoreNLP\n",
      "plainTextToLemmas: (text: String, stopWords: Set[String], pipeline: edu.stanford.nlp.pipeline.StanfordCoreNLP)Seq[String]\n",
      "documentTermMatrix: (docTexts: org.apache.spark.sql.Dataset[(String, String)], stopWordsFile: String, numTerms: Int)(org.apache.spark.sql.DataFrame, Array[String], Map[Long,String], Array[Double])\n"
     ]
    }
   ],
   "source": [
    "def createNLPPipeline(): StanfordCoreNLP = {\n",
    "    val props = new Properties()\n",
    "    props.put(\"annotators\", \"tokenize, ssplit, pos, lemma\")\n",
    "    new StanfordCoreNLP(props)    \n",
    "}\n",
    "\n",
    "def plainTextToLemmas(text:String, stopWords: Set[String], pipeline: StanfordCoreNLP): Seq[String] = {\n",
    "        \n",
    "    val doc = new Annotation(text)\n",
    "    \n",
    "    pipeline.annotate(doc)\n",
    "    \n",
    "    val lemmas = new ArrayBuffer[String]()\n",
    "    val sentences = doc.get(classOf[SentencesAnnotation])\n",
    "    for (sentence <- sentences.asScala; \n",
    "         token <- sentence.get(classOf[TokensAnnotation]).asScala) {\n",
    "        val lemma = token.get(classOf[LemmaAnnotation])\n",
    "        if (lemma.length > 2 && !stopWords.contains(lemma)) {\n",
    "            lemmas += lemma.toLowerCase\n",
    "        }\n",
    "    }\n",
    "    lemmas\n",
    "}\n",
    "\n",
    "/**\n",
    "* Returns a document-term matrix where each element is the TF-IDF of the row's document and\n",
    "* the column's term.\n",
    "*\n",
    "* @param docTexts a DF with two columns: title and text\n",
    "* @return : DocTermMatrix,  termIds(voca) docIds(id -> title), termIdfs (단어별 IDF)\n",
    "*/\n",
    "def documentTermMatrix(docTexts: Dataset[(String, String)], stopWordsFile: String, numTerms: Int)\n",
    "    : (DataFrame, Array[String], Map[Long, String], Array[Double]) = {\n",
    "        \n",
    "    val stopWords = sc.textFile(base + \"stopwords.txt\").collect.toSet\n",
    "    val bStopWords = sc.broadcast(stopWords)    \n",
    "\n",
    "    // document's title -> seq of lemma\n",
    "    val terms: Dataset[(String, Seq[String])] = \n",
    "        docTexts.mapPartitions { iter =>\n",
    "            val pipeline = createNLPPipeline()\n",
    "            iter.map { case (title, contents) =>\n",
    "                (title, plainTextToLemmas(contents, bStopWords.value, pipeline))\n",
    "            }\n",
    "        }\n",
    " \n",
    "    val termsDF = terms.toDF(\"title\", \"terms\").withColumn(\"size\", size($\"terms\")).filter($\"size\" > 1)\n",
    "    \n",
    "    val countVectorizer = new CountVectorizer().\n",
    "        setInputCol(\"terms\").\n",
    "        setOutputCol(\"termFreqs\").\n",
    "        setVocabSize(numTerms)\n",
    "        \n",
    "    val vocaModel = countVectorizer.fit(termsDF)\n",
    "    val termIds: Array[String] = vocaModel.vocabulary\n",
    "        \n",
    "    val docTermFreqs:DataFrame = vocaModel.transform(termsDF)\n",
    "    \n",
    "    // 문서 title -> unique id\n",
    "    val docIds = termsDF.rdd.\n",
    "        map( _.getString(0)).\n",
    "        zipWithUniqueId().\n",
    "        map( _.swap).\n",
    "        collect().toMap\n",
    "        \n",
    "    val idf = new IDF().\n",
    "        setInputCol(\"termFreqs\").\n",
    "        setOutputCol(\"tfidfVec\")\n",
    "    val idfModel = idf.fit(docTermFreqs)\n",
    "        \n",
    "    val docTermMatrix:DataFrame = idfModel.transform(docTermFreqs)        \n",
    "        \n",
    "    (docTermMatrix, termIds, docIds, idfModel.idf.toArray)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documentTermMatrix: (docTexts: org.apache.spark.sql.Dataset[(String, String)], stopWordsFile: String, numTerms: Int)(org.apache.spark.sql.DataFrame, Array[String], Map[Long,String], Array[Double])\n",
      "stopWordFilePath: String = hdfs://localhost:54310/wikipedia/stopwords.txt\n",
      "docTermMatrix: org.apache.spark.sql.DataFrame = [title: string, terms: array<string> ... 3 more fields]\n",
      "termIds: Array[String] = Array(-rrb-, -lrb-, -rcb-, -lcb-, point, space, line, can, state, geometry, use, government, group, one, also, two, plane, frac, mathcal, form, right, set, energy, give, surface, angle, system, time, vector, define, may, example, function, case, see, emergency, shape, -lsb-, -rsb-, theory, call, cdot, metric, first, include, number, field, coordinate, public, follow, triangle, term, universe, distance, phi, model, area, image, theorem, method, operatorname, symmetry, equation, general, structure, end, base, unit, map, object, axis, parallel, circle, mathbf, minkowski, order, many, law, mathbb, three, reference, geodesic, dimension, make, begin, transformation,...res83: docTermMatrix.type = [title: string, terms: array<string> ... 3 more fields]\n"
     ]
    }
   ],
   "source": [
    "val stopWordFilePath = base + \"stopwords.txt\"\n",
    "\n",
    "val (docTermMatrix, termIds, docIds, termIdfs) = documentTermMatrix(docTexts, stopWordFilePath, 20000)\n",
    "docTermMatrix.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+--------------------+--------------------+\n",
      "|               title|               terms|size|           termFreqs|            tfidfVec|\n",
      "+--------------------+--------------------+----+--------------------+--------------------+\n",
      "|              Energy|[energy, physics,...|3821|(16310,[0,1,2,3,4...|(16310,[0,1,2,3,4...|\n",
      "|            Universe|[universe, univer...|5449|(16310,[0,1,2,3,4...|(16310,[0,1,2,3,4...|\n",
      "|       Ambient space|[ambient, space, ...| 137|(16310,[0,1,2,3,5...|(16310,[0,1,2,3,5...|\n",
      "|          Superspace|[superspace, supe...|1191|(16310,[0,1,2,3,4...|(16310,[0,1,2,3,4...|\n",
      "|     Geometry Center|[geometry, center...| 364|(16310,[0,1,5,7,8...|(16310,[0,1,5,7,8...|\n",
      "|          Dehn plane|[dehn, plane, geo...| 248|(16310,[0,1,2,3,4...|(16310,[0,1,2,3,4...|\n",
      "|Complex reflectio...|[complex, reflect...|1932|(16310,[0,1,2,3,5...|(16310,[0,1,2,3,5...|\n",
      "|    Lipschitz domain|[lipschitz, domai...| 289|(16310,[0,1,2,3,4...|(16310,[0,1,2,3,4...|\n",
      "|        Complex line|[complex, line, m...|  43|(16310,[0,1,4,5,6...|(16310,[0,1,4,5,6...|\n",
      "|Visibility (geome...|[visibility, -lrb...| 120|(16310,[0,1,4,5,6...|(16310,[0,1,4,5,6...|\n",
      "|   Spacetime diagram|[spacetime, diagr...|2350|(16310,[0,1,2,3,4...|(16310,[0,1,2,3,4...|\n",
      "|Partial linear space|[partial, linear,...| 168|(16310,[0,1,2,3,4...|(16310,[0,1,2,3,4...|\n",
      "| Geometry processing|[geometry, proces...|2105|(16310,[0,1,2,3,4...|(16310,[0,1,2,3,4...|\n",
      "|          Hatch mark|[hatch, mark, hat...| 353|(16310,[0,1,4,6,7...|(16310,[0,1,4,6,7...|\n",
      "|       Lattice plane|[lattice, plane, ...|  95|(16310,[0,1,4,7,8...|(16310,[0,1,4,7,8...|\n",
      "|Tarski's plank pr...|[tarski, plank, p...| 178|(16310,[0,1,2,3,4...|(16310,[0,1,2,3,4...|\n",
      "|     Visual calculus|[visual, calculus...| 348|(16310,[0,1,4,6,7...|(16310,[0,1,4,6,7...|\n",
      "|          Benz plane|[benz, plane, mat...| 411|(16310,[0,1,2,3,4...|(16310,[0,1,2,3,4...|\n",
      "|Infinitely near p...|[infinitely, near...| 254|(16310,[0,1,2,3,4...|(16310,[0,1,2,3,4...|\n",
      "| Axis-aligned object|[axis-aligned, ob...|  78|(16310,[0,1,5,6,9...|(16310,[0,1,5,6,9...|\n",
      "+--------------------+--------------------+----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docTermMatrix.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특이값 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
      "convertToOldVectors: (df: org.apache.spark.sql.DataFrame)org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector]\n",
      "k: Int = 10\n",
      "vecRdd: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[401] at map at <console>:108\n",
      "mat: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@d5e4cc4\n",
      "svd: org.apache.spark.mllib.linalg.SingularValueDecomposition[org.apache.spark.mllib.linalg.distributed.RowMatrix,org.apache.spark.mllib.linalg.Matrix] =\n",
      "SingularValueDecomposition(org.apache.spark.mllib.linalg.distributed.RowMatrix@420d0714,[1428.1048884264724,1181.1939773221345,979.0249272611959,806.6422672096115,694.581972452828,655.7450400535882,638.984364351144,619.4012372879052,580.2711528203106,564.5549731288947],-0.007564968081232696   -0.03720766127091363   ... (10 total)\n",
      "-0.007545286282372311   -0.03678584426480323   ...\n",
      "-0.021130134071485722   -0.45442671966358994   ...\n",
      "-0.020941527998448832   -0.45046351338292334   ...\n",
      "-0.002049141798297606   -0.029789700328074334  ...\n",
      "-0.005048413096711068   -0.07521815561742476   ...\n",
      "-0.0018244865634585022  -0.018485389872449957  ...\n",
      "-0.01...[1428.1048884264724,1181.1939773221345,979.0249272611959,806.6422672096115,694.581972452828,655.7450400535882,638.984364351144,619.4012372879052,580.2711528203106,564.5549731288947]\n"
     ]
    }
   ],
   "source": [
    "// spark.mllib의 vectorRDD로 변환\n",
    "def convertToOldVectors(df:DataFrame) = {\n",
    "    // Old Migration Guides - MLlib : https://spark.apache.org/docs/2.2.0/ml-migration-guides.html\n",
    "    \n",
    "    df.select(\"tfidfVec\").rdd.map { row =>\n",
    "        Vectors.fromML(row.getAs[MLVector](\"tfidfVec\"))\n",
    "    }\n",
    "}\n",
    "\n",
    "val k = 10\n",
    "val vecRdd = convertToOldVectors(docTermMatrix)\n",
    "\n",
    "val mat: RowMatrix = new RowMatrix(vecRdd) \n",
    "val svd = mat.computeSVD(k, computeU = true)\n",
    "\n",
    "println(svd.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중요한 단어/문서 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "code_folding": [
     7,
     31
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topTermsInTopConcepts: (svd: org.apache.spark.mllib.linalg.SingularValueDecomposition[org.apache.spark.mllib.linalg.distributed.RowMatrix,org.apache.spark.mllib.linalg.Matrix], numConcepts: Int, numTerms: Int, termIds: Array[String])Seq[Seq[(String, Double)]]\n",
      "topDocInTopConcepts: (svd: org.apache.spark.mllib.linalg.SingularValueDecomposition[org.apache.spark.mllib.linalg.distributed.RowMatrix,org.apache.spark.mllib.linalg.Matrix], numConcepts: Int, numDocs: Int, docIds: Map[Long,String])Seq[Seq[(String, Double)]]\n",
      "topTerms: Seq[Seq[(String, Double)]] = ArrayBuffer(ArraySeq((emergency,-0.7465818357441569), (declare,-0.349791133077605), (state,-0.1946150873436259)), ArraySeq((-rcb-,-0.45442671966358994), (-lcb-,-0.45046351338292334), (anatomy,-0.15676855801555575)), ArraySeq((-rcb-,-0.13117486016758945), (-lcb-,-0.1301037087462227), (mathcal,-0.051971094231352086)))\n",
      "topDocs: Seq[Seq[(String, Double)]] = ArrayBuffer(ArraySeq((Manipulability ellipsoid,-2.498175128185344E-5), (Flatness (mathematics),-4.861098501115765E-5), (Badouel intersection algorithm,-5.53180647758757E-5)), ArraySeq((State of emergency,0.06804263857248531), (Government district,-2.179792852128997E-5), (Cabinet crisis,-1.0386326957809018E-4)), ArraySeq((Energy,0.6812270482469955), (Universe,0.6779231932279577), (Space,0.04517499553732213)))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "// 중요 의미를 갖는 최상위 단어 목록 \n",
    "//  V에서 각 의미별 topN (단어, 중요도) 목록 \n",
    "def topTermsInTopConcepts(\n",
    "    svd: SingularValueDecomposition[RowMatrix, Matrix],\n",
    "    numConcepts: Int, \n",
    "    numTerms: Int,\n",
    "    termIds:Array[String]\n",
    ") : Seq[Seq[(String, Double)]] = {\n",
    "       \n",
    "    val v = svd.V\n",
    "    val arr = v.toArray\n",
    "    \n",
    "    // output\n",
    "    val topTerms = new ArrayBuffer[Seq[(String, Double)]]()\n",
    "    \n",
    "    for ( i <- 0 until numConcepts ) {\n",
    "        val offset = i * v.numRows       \n",
    "        topTerms += arr.slice(offset, offset+v.numRows).zipWithIndex.\n",
    "            sortBy(_._1).take(numTerms).map{ case (score, id) => (termIds(id), score) } \n",
    "    }\n",
    "    \n",
    "    topTerms\n",
    "}\n",
    "\n",
    "// 중요 의미를 갖는 최상위 문서 목록\n",
    "//  U 에서 각 의미별 topN(문서, 중요도) 목록 \n",
    "def topDocInTopConcepts(\n",
    "    svd: SingularValueDecomposition[RowMatrix, Matrix],\n",
    "    numConcepts: Int, \n",
    "    numDocs: Int,\n",
    "    docIds:Map[Long,String]\n",
    ") : Seq[Seq[(String, Double)]] = {\n",
    " \n",
    "    val u = svd.U\n",
    "    \n",
    "    // output\n",
    "    val topTerms = new ArrayBuffer[Seq[(String, Double)]]()\n",
    "    \n",
    "    for ( i <- 0 until numConcepts ) {\n",
    "        topTerms += u.rows.map(row => row.toArray(i)).zipWithUniqueId().sortBy(_._1).top(numDocs).map { \n",
    "            case (score, id) => (docIds(id), score)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    topTerms\n",
    "}\n",
    "\n",
    "val topTerms:Seq[Seq[(String, Double)]]  = topTermsInTopConcepts(svd, numConcepts = 3, numTerms=3, termIds=termIds)\n",
    "val topDocs:Seq[Seq[(String, Double)]]   = topDocInTopConcepts(svd, numConcepts = 3, numDocs=3, docIds=docIds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  연관도 계산\n",
    "\n",
    "* 단어와 단어 사이의 연관도 \n",
    "  * V를 S를 고려해 normalize 하고 query 단어와 cosine similiary 기반 유사도 측정\n",
    "    * (V_t * S) * query_v\n",
    "* 문서와 문서 사이의 연관도\n",
    "  * U를 S를 고려해 정규화하고 query 문서와 cosine similiaty 기반 유사도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "code_folding": [
     17,
     27,
     34,
     45,
     84,
     97,
     111,
     120
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined class LSAQueryEngine\n"
     ]
    }
   ],
   "source": [
    "class LSAQueryEngine(svd: SingularValueDecomposition[RowMatrix, Matrix],\n",
    "                     termIds:Array[String],\n",
    "                     docIds:Map[Long,String],\n",
    "                     termIdfs: Array[Double]\n",
    "                    ) {\n",
    "\n",
    "    import breeze.linalg.{DenseMatrix => BDenseMatrix, SparseVector => BSparseVector}\n",
    "    \n",
    "    val idTerms:Map[String, Int] = termIds.zipWithIndex.toMap\n",
    "    val idDocs: Map[String, Long] = docIds.map(_.swap).toMap\n",
    "    \n",
    "    val VS: BDenseMatrix[Double] = multiplyByDiagonalMatrix(svd.V, svd.s)\n",
    "    val normalizedVS: BDenseMatrix[Double] = rowsNormalized(VS)\n",
    "    val US: RowMatrix = multiplyByDiagonalRowMatrix(svd.U, svd.s)\n",
    "    val normalizedUS: RowMatrix = distributedRowsNormalized(US)\n",
    "    \n",
    "    // 하나의 단어와 연관된 단어들 찾기 \n",
    "    def topTermsForTerm(termId:Int): Seq[(Double, Int)] =  {\n",
    "        \n",
    "        val rowVec = normalizedVS(termId, ::).t\n",
    "        \n",
    "        val termScores = (normalizedVS * rowVec).toArray.zipWithIndex\n",
    "                \n",
    "        termScores.sortBy(-_._1).take(10)\n",
    "    }\n",
    "\n",
    "    // 하나의 단어와 연관된 단어들 정보 출력하기\n",
    "    def printTopTermsForTerm(term:String): Unit = {\n",
    "        topTermsForTerm(idTerms(term)).map { case (score, id) =>\n",
    "             println(termIds(id), score)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // 하나의 문서와 연관된 문서들 찾기 \n",
    "    def topDocsForDoc(docId:Long): Seq[(Double, Long)] = {\n",
    "        \n",
    "        val docRowArray = normalizedUS.rows.zipWithUniqueId.map(_.swap).lookup(docId).head.toArray\n",
    "        val docRowVec = Matrices.dense(docRowArray.length, 1, docRowArray)\n",
    "        \n",
    "        val docScores = normalizedUS.multiply(docRowVec)\n",
    "        \n",
    "        docScores.rows.map(_.toArray(0)).zipWithUniqueId().filter(!_._1.isNaN).top(10)        \n",
    "    }\n",
    "    \n",
    "    // 하나의 문서와 연관된 문서들 정보 출력하기\n",
    "    def printTopDocsForDoc(doc:String): Unit = {\n",
    "        topDocsForDoc(idDocs(doc)).map { case (score, docId) =>\n",
    "            println( docIds(docId), score )\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def termsToQueryVector(terms:Seq[String]): BSparseVector[Double] = {\n",
    "        val indices = terms.map(idTerms(_)).toArray\n",
    "        val values = indices.map(termIdfs(_))\n",
    "        new BSparseVector[Double](indices, values, idTerms.size)\n",
    "    }\n",
    "    \n",
    "    def topDocsForTermQuery(query: BSparseVector[Double]): Seq[(Double, Long)] = {\n",
    "        \n",
    "        val v = svd.V\n",
    "        val breezeV = new BDenseMatrix[Double](v.numRows, v.numCols, v.toArray)\n",
    "        \n",
    "        val termRowArr = (breezeV.t * query).toArray\n",
    "        \n",
    "        val termRowVec = Matrices.dense(termRowArr.length, 1, termRowArr)\n",
    "        \n",
    "        val docScores = US.multiply(termRowVec)\n",
    "        \n",
    "        docScores.rows.map(_.toArray(0)).zipWithUniqueId().top(10)\n",
    "    }\n",
    "    \n",
    "    def printTopDocsForTermQuery(terms: Seq[String]):Unit = {\n",
    "        \n",
    "        val queryVector = termsToQueryVector(terms)\n",
    "        val topDocs = topDocsForTermQuery(queryVector)\n",
    "        \n",
    "        topDocs.map { case (score, docId) =>\n",
    "            println( docIds(docId), score )\n",
    "        }\n",
    "    \n",
    "    }\n",
    "    \n",
    "    /**\n",
    "    * Returns a matrix where each row is divided by its length.\n",
    "    */\n",
    "    def rowsNormalized(mat: BDenseMatrix[Double]): BDenseMatrix[Double] = {\n",
    "        val newMat = new BDenseMatrix[Double](mat.rows, mat.cols)\n",
    "        for (r <- 0 until mat.rows) {\n",
    "            val length = math.sqrt((0 until mat.cols).map(c => mat(r, c) * mat(r, c)).sum)\n",
    "            (0 until mat.cols).foreach(c => newMat.update(r, c, mat(r, c) / length))\n",
    "        }\n",
    "        newMat\n",
    "    }\n",
    "    \n",
    "    \n",
    "    /**\n",
    "    * Returns a distributed matrix where each row is divided by its length.\n",
    "    */\n",
    "    def distributedRowsNormalized(mat: RowMatrix): RowMatrix = {\n",
    "        new RowMatrix(\n",
    "            mat.rows.map { vec =>\n",
    "                val array = vec.toArray\n",
    "                val length = math.sqrt(array.map(x => x * x).sum)\n",
    "                Vectors.dense(array.map(_ / length))\n",
    "            }\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    /**\n",
    "    * Finds the product of a dense matrix and a diagonal matrix represented by a vector.\n",
    "    * Breeze doesn't support efficient diagonal representations, so multiply manually.\n",
    "    */\n",
    "    def multiplyByDiagonalMatrix(mat: Matrix, diag: MLLibVector): BDenseMatrix[Double] = {\n",
    "        val sArr = diag.toArray\n",
    "        new BDenseMatrix[Double](mat.numRows, mat.numCols, mat.toArray)\n",
    "          .mapPairs { case ((r, c), v) => v * sArr(c) }\n",
    "    }\n",
    "\n",
    "    /**\n",
    "    * Finds the product of a distributed matrix and a diagonal matrix represented by a vector.\n",
    "    */\n",
    "    def multiplyByDiagonalRowMatrix(mat: RowMatrix, diag: MLLibVector): RowMatrix = {\n",
    "        val sArr = diag.toArray\n",
    "        new RowMatrix(\n",
    "            mat.rows.map { vec =>\n",
    "                val vecArr = vec.toArray\n",
    "                val newArr = (0 until vec.size).toArray.map(i => vecArr(i) * sArr(i))\n",
    "                Vectors.dense(newArr)\n",
    "            }\n",
    "        )\n",
    "    }    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: LSAQueryEngine = LSAQueryEngine@1d25bafc\n"
     ]
    }
   ],
   "source": [
    "val engine = new LSAQueryEngine(svd,termIds, docIds, termIdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(energy,0.9999999999999999)\n",
      "(nuclear,0.9972423093067236)\n",
      "(fusion,0.9969145023661303)\n",
      "(heat,0.9958377414805573)\n",
      "(thermal,0.995201154296173)\n",
      "(carbon,0.9944547724612467)\n",
      "(chemical,0.9943576275841175)\n",
      "(substance,0.992554424627008)\n",
      "(potential,0.9915418851818423)\n",
      "(release,0.9911710444441805)\n"
     ]
    }
   ],
   "source": [
    "engine.printTopTermsForTerm(\"energy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Dual mandate,0.9999999999999998)\n",
      "(Criticism of government,0.9947118707974583)\n",
      "(Caretaker government,0.9941160444452173)\n",
      "(Legislative calendar,0.9892313159816131)\n",
      "(Ministry (collective executive),0.9865869905145708)\n",
      "(Administrative centre,0.9853372013300937)\n",
      "(Outline of government,0.9803624502900843)\n",
      "(Legislative session,0.979618539588496)\n",
      "(Governmental accounting,0.9780341254196077)\n",
      "(Center of government,0.9780082332970367)\n"
     ]
    }
   ],
   "source": [
    "engine.printTopDocsForDoc(\"Dual mandate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Energy,22.70955453211654)\n",
      "(Minkowski space,14.171607100078283)\n",
      "(Busemann function,13.849470796186438)\n",
      "(Computational anatomy,13.079551933027782)\n",
      "(Universe,12.089365968679232)\n",
      "(Line moiré,6.04189992759229)\n",
      "(State of emergency,5.913983032091663)\n",
      "(Fat object (geometry),5.878769726826015)\n",
      "(Schema for horizontal dials,5.862831640520457)\n",
      "(Spacetime diagram,5.853655764740569)\n"
     ]
    }
   ],
   "source": [
    "engine.printTopDocsForTermQuery(Seq(\"quantitative\", \"property\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
