{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 스파크 고급 분석 9장\n",
    "\n",
    "## 몬테카를로 시뮬레이션으로 금융 리스크 추정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'mc-finance', 'proxyUser': 'hduser', 'driverMemory': '4000M', 'conf': {'spark.jars.packages': 'graphframes:graphframes:0.3.0-spark2.0-s_2.11', 'spark.master': 'local[2]', 'spark.jars': 'hdfs://localhost:54310/jars/ch06-lsa-2.0.0-jar-with-dependencies.jar', 'spark.sql.crossJoin.enabled': 'true'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"name\": \"mc-finance\",\n",
    "    \"proxyUser\": \"hduser\",\n",
    "    \"driverMemory\": \"4000M\", \n",
    "    \"conf\": {\"spark.jars.packages\": \"graphframes:graphframes:0.3.0-spark2.0-s_2.11\",\n",
    "             \"spark.master\": \"local[2]\",\n",
    "             \"spark.jars\": \"hdfs://localhost:54310/jars/ch06-lsa-2.0.0-jar-with-dependencies.jar\",\n",
    "             \"spark.sql.crossJoin.enabled\": \"true\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import java.time.LocalDate\n",
      "import java.time.format.DateTimeFormatter\n",
      "import java.time.format.DateTimeFormatterBuilder\n",
      "import java.io.File\n",
      "import java.util.Locale\n",
      "import scala.collection.mutable.ArrayBuffer\n",
      "import org.apache.commons.math3.distribution.ChiSquaredDistribution\n",
      "import org.apache.commons.math3.distribution.MultivariateNormalDistribution\n",
      "import org.apache.commons.math3.random.MersenneTwister\n",
      "import org.apache.commons.math3.stat.correlation.Covariance\n",
      "import org.apache.commons.math3.stat.regression.OLSMultipleLinearRegression\n",
      "import org.apache.spark.sql.Dataset\n"
     ]
    }
   ],
   "source": [
    "import java.time.LocalDate\n",
    "import java.time.format.DateTimeFormatter\n",
    "import java.time.format.DateTimeFormatterBuilder\n",
    "import java.io.File\n",
    "import java.util.Locale\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "//import breeze.plot._\n",
    "import org.apache.commons.math3.distribution.ChiSquaredDistribution\n",
    "import org.apache.commons.math3.distribution.MultivariateNormalDistribution\n",
    "import org.apache.commons.math3.random.MersenneTwister\n",
    "import org.apache.commons.math3.stat.correlation.Covariance\n",
    "import org.apache.commons.math3.stat.regression.OLSMultipleLinearRegression\n",
    "import org.apache.spark.sql.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_HOME: String = /home/hoondori/data/\n",
      "base: String = hdfs://localhost:54310/stock/\n",
      "localBase: String = /home/hoondori/data/stock\n"
     ]
    }
   ],
   "source": [
    "val DATA_HOME = \"/home/hoondori/data/\"\n",
    "val base = \"hdfs://localhost:54310/stock/\"\n",
    "val localBase = DATA_HOME + \"stock\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overiew\n",
    "\n",
    "최대손실예상액(VaR: Value at Risk)이나 조건부 VaR 최소화가 목표이다. \n",
    "\n",
    "VaR 계산 방법\n",
    "* 분산-공분산 : 각 금융상품의 수익을 정규분포라고 가정하고 단순 계산\n",
    "* 과거 시뮬레이션 : 요약 통계가 아닌 과거 데이터(금융상품별 주가 raw 그래프)의 분포로 직접 추정\n",
    "* MC 시뮬레이션 - 가상 시뮬레이션을 반복해서 수행/관찰 => 확률 밀도 함수 추정 \n",
    "  1. 시장 요인과 각 금융상품의 수익간의 모델링 (Linear Model)\n",
    "  1. 시장 요인들의 생성 분포 정의 (fit to historical data) - 다변량 정규 분포 가정 with Non-diagonal cov\n",
    "  1. 임의 시장 요인들로 구성된 실험들 제기 \n",
    "  1. 각 실험에서 전체 포트폴리오의 손실 계산 => 손실에 대한 경험적 분포 정의 \n",
    "  1. 손실 분포에서 n% Var 추정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     2,
     23,
     38,
     62,
     72
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<console>:34: warning: This catches all Throwables. If this is really intended, use `case _ : Throwable` to clear this warning.\n",
      "       def parseDouble(s: String) = try { Some(s.toDouble) } catch { case _ => None }\n",
      "                                                                          ^\n",
      "parseDouble: (s: String)Option[Double]\n",
      "warning: there was one feature warning; re-run with -feature for details\n",
      "readYahooHistory: (file: java.io.File)Array[(java.time.LocalDate, Double)]\n",
      "trimToRegion: (history: Array[(java.time.LocalDate, Double)], start: java.time.LocalDate, end: java.time.LocalDate)Array[(java.time.LocalDate, Double)]\n",
      "fillInHistory: (history: Array[(java.time.LocalDate, Double)], start: java.time.LocalDate, end: java.time.LocalDate)Array[(java.time.LocalDate, Double)]\n",
      "twoWeekReturns: (history: Array[(java.time.LocalDate, Double)])Array[Double]\n",
      "readStocksAndFactors: ()(Seq[Array[Double]], Seq[Array[Double]])\n",
      "stocks: Seq[Array[Double]] = Stream([D@3096a2a4, ?)\n",
      "factors: Seq[Array[Double]] = WrappedArray(Array(0.02210526315789476, 0.0383908712890436, 0.0667386397094906, 0.06433497536945813, 0.07638615808100664, 0.06674484510896118, 0.04657004830917868, 0.06709080393290928, 0.03715806180562043, 0.05505102518490774, 0.04162786366176196, 0.02806145919587816, 0.06146440803480515, 0.047858251895150326, 0.05982044705020155, 0.06803914327917286, 0.02149954832881658, 0.030515798001620312, 0.040908687549915695, 0.062136790344211025, 0.061124037945229984, 0.021627278276794313, -0.028065893837705906, -0.025326303051257607, -0.0353530987985133, -0.021400778210116746, -0.033892382948986804, -0.06214833759590786, -0.07457912457912458, -0.06527789491439652, -0.06555697823303462, -0.03147699757869254, -0.0318...res10: Boolean = true\n"
     ]
    }
   ],
   "source": [
    "def parseDouble(s: String) = try { Some(s.toDouble) } catch { case _ => None }\n",
    "\n",
    "def readYahooHistory(file:File): Array[(LocalDate, Double)] = {\n",
    "    val df = new DateTimeFormatterBuilder().\n",
    "        // case insensitive to parse JAN and FEB\n",
    "        parseCaseInsensitive().\n",
    "        // add pattern\n",
    "        appendPattern(\"d-MMM-yy\").\n",
    "        // create formatter (use English Locale to parse month names)\n",
    "        toFormatter(Locale.ENGLISH)\n",
    "\n",
    "    val lines = scala.io.Source.fromFile(file).getLines().toSeq \n",
    "    (lines.tail.map { line =>\n",
    "        val cols = line.split(',')\n",
    "        val date = LocalDate.parse(cols(0), df)\n",
    "        (date, parseDouble(cols(1)))\n",
    "    } filter { case ( date, valOpt) =>\n",
    "        valOpt.isDefined\n",
    "    } map { case (date, valOpt) =>\n",
    "        (date, valOpt.get)\n",
    "    } reverse).toArray \n",
    "}\n",
    "\n",
    "def trimToRegion(history: Array[(LocalDate, Double)], start:LocalDate, end:LocalDate): Array[(LocalDate, Double)] = {\n",
    "    \n",
    "    // 기간 내의 정보만 trim\n",
    "    var trimmed = history.dropWhile(_._1.isBefore(start)).takeWhile(x => x._1.isBefore(end) || x._1.isEqual(end))\n",
    "    \n",
    "    // 시작/끝 날짜가 없으면 fill\n",
    "    if (trimmed.head._1 != start) {\n",
    "        trimmed = Array((start, trimmed.head._2)) ++ trimmed\n",
    "    }\n",
    "    if (trimmed.last._1 != end) {\n",
    "        trimmed = trimmed ++ Array((end, trimmed.last._2))\n",
    "    }\n",
    "    trimmed\n",
    "}\n",
    "\n",
    "def fillInHistory(history: Array[(LocalDate, Double)], start:LocalDate, end:LocalDate): Array[(LocalDate, Double)] = {\n",
    "    var cur = history\n",
    "    \n",
    "    val filled = new ArrayBuffer[(LocalDate, Double)]\n",
    "    \n",
    "    var curDate = start\n",
    "    while(curDate.isBefore(end)) {\n",
    "        if(cur.tail.nonEmpty && cur.tail.head._1.isEqual(curDate)) { \n",
    "            cur = cur.tail\n",
    "        }\n",
    "        \n",
    "        filled += ((curDate, cur.head._2))\n",
    "        \n",
    "        curDate = curDate.plusDays(1)\n",
    "        \n",
    "        // 주말 제외\n",
    "        if (curDate.getDayOfWeek.getValue > 5) {\n",
    "            curDate = curDate.plusDays(2)\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    filled.toArray\n",
    "}\n",
    "\n",
    "def twoWeekReturns(history: Array[(LocalDate, Double)]): Array[Double] = {\n",
    "    \n",
    "    history.sliding(10).   // 10일 간격\n",
    "        map { window =>\n",
    "            val startPrice = window.head._2\n",
    "            val endPrice = window.last._2\n",
    "            ((endPrice - startPrice)) / startPrice\n",
    "        }.toArray\n",
    "}\n",
    "\n",
    "def readStocksAndFactors(): (Seq[Array[Double]], Seq[Array[Double]]) = {\n",
    "\n",
    "    val start = LocalDate.of(2009, 10, 23)\n",
    "    val end = LocalDate.of(2014, 10, 23)\n",
    "\n",
    "    // stock 확보 \n",
    "    val stockDir = new File(localBase + \"/stocks/\")\n",
    "    val files = stockDir.listFiles()\n",
    "    val allStocks = files.iterator.flatMap { file =>\n",
    "        try {\n",
    "            Option(readYahooHistory(file))    \n",
    "        } catch {\n",
    "            case e: Exception => None\n",
    "        }\n",
    "    }\n",
    "    val rawStocks = allStocks.filter(_.size >= 260*5 +10)\n",
    "    val stocks = rawStocks.\n",
    "        map(trimToRegion(_, start, end)).\n",
    "        map(fillInHistory(_, start, end))\n",
    "    \n",
    "    // factor 확보\n",
    "    val factorDir = localBase + \"/factors/\"\n",
    "    val rawFactors = Array(\"NYSEARCA%3AGLD.csv\", \"NASDAQ%3ATLT.csv\", \"NYSEARCA%3ACRED.csv\").map { x =>\n",
    "        new File(factorDir + x)\n",
    "    }.map { file =>\n",
    "        readYahooHistory(file)\n",
    "    }\n",
    "    val factors = rawFactors.\n",
    "        map(trimToRegion(_, start, end)).\n",
    "        map(fillInHistory(_, start, end))\n",
    "\n",
    "    val stockReturns = stocks.map(twoWeekReturns).toSeq\n",
    "    val factorReturns = factors.map(twoWeekReturns).toSeq\n",
    "    (stockReturns, factorReturns)\n",
    "}\n",
    "\n",
    "val (stocks, factors) = readStocksAndFactors()\n",
    "\n",
    "// check\n",
    "(stocks ++ factors).forall(_.size == stocks(0).size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시장 요인 가중치 결정하기 (LinearModel Fitting)\n",
    "\n",
    "* 각 상품별 two week returen 이 y값\n",
    "* 모든 기간의 factor가 DataMatrix x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     1,
     9,
     18
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factorMatrix: (histories: Seq[Array[Double]])Array[Array[Double]]\n",
      "featurize: (factorReturns: Array[Double])Array[Double]\n",
      "linearModel: (instrument: Array[Double], factorMatrix: Array[Array[Double]])org.apache.commons.math3.stat.regression.OLSMultipleLinearRegression\n",
      "factorMat: Array[Array[Double]] = Array(Array(0.02210526315789476, -0.02118465430016854, -3.9572615749893197E-4), Array(0.0383908712890436, -0.01122405760271074, 0.00405980790177256), Array(0.0667386397094906, -0.007342768968819812, 0.010596157654981116), Array(0.06433497536945813, -0.011456800504519691, 0.004737465455981089), Array(0.07638615808100664, -0.012550094916684221, 0.002857424376785952), Array(0.06674484510896118, -0.0195583596214511, -9.81932443048002E-4), Array(0.04657004830917868, -0.01758978117474617, 0.002164715143166377), Array(0.06709080393290928, -0.006612090680100708, 0.005812807881773433), Array(0.03715806180562043, 0.014742014742014694, 0.007382616399251894), Array(0.05505102518490774, 0.027673091418111265, 0.015043547110055384), Array(0.04162786366176196, 0.019918...factorFeatures: Array[Array[Double]] = Array(Array(4.886426592797794E-4, -4.487895778176494E-4, -1.565991917288695E-7, 0.1486783883350057, -0.14554949089628771, -0.019892867000483667, 0.02210526315789476, -0.02118465430016854, -3.9572615749893197E-4), Array(0.0014738589983319123, -1.2597946906896875E-4, 1.6482040199294915E-5, 0.19593588565917067, -0.1059436529609525, 0.06371662186409885, 0.0383908712890436, -0.01122405760271074, 0.00405980790177256), Array(0.004454046030273195, -5.391625612946317E-5, 1.1227855704921491E-4, 0.2583382273483555, -0.0856899583896492, 0.10293763964158648, 0.0667386397094906, -0.007342768968819812, 0.010596157654981116), Array(0.004138989055788785, -1.3125827780036267E-4, 2.244357894661411E-5, 0.2536434019828983, -0.10703644474906522, 0.06882924855017007, 0.0...factorWeights: Array[Array[Double]] = Array(Array(0.00855754738957922, 4.339468720703037, -2.709859908760106, 24.58081816130914, 0.11059702322979355, -0.09345770476728359, -0.025219390795904136, -0.46260468205106486, 0.133614077466534, -0.33212888920309513), Array(0.003562394075097068, -0.4937552437056755, 3.763106194081878, -21.206263477892026, 0.08986396424560669, -0.02104300427506067, -0.04265595403339369, -0.2544340462521001, -1.6532783492645933, 2.987745562747737), Array(0.0036700288701406957, 5.001706520473336, -18.998151863924306, 404.5053799624183, 0.04488809233741747, -0.22149714823367603, 0.6966237652311914, -0.1868631146041032, -0.0746656878019527, -8.141405720033898), Array(0.0045104324279174315, -4.2219442762207064, 0.4986270668317386, -96.25438111310224, 0.0084552912262898...res15: Int = 1688\n",
      "res16: Int = 1774\n"
     ]
    }
   ],
   "source": [
    "// transpose\n",
    "def factorMatrix(histories: Seq[Array[Double]]): Array[Array[Double]] = {\n",
    "    val mat = new Array[Array[Double]](histories.head.length)\n",
    "    for (i <- histories.head.indices) {\n",
    "        mat(i) = histories.map(_(i)).toArray\n",
    "    }\n",
    "    mat\n",
    "}\n",
    "\n",
    "def featurize(factorReturns: Array[Double]): Array[Double] = {\n",
    "    // x ++ x^2 ++ sqrt_x\n",
    "\n",
    "    val squaredReturns = factorReturns.map(x => math.signum(x) * x * x)\n",
    "    val squareRootedReturns = factorReturns.map(x => math.signum(x) * math.sqrt(math.abs(x)))\n",
    "    squaredReturns ++ squareRootedReturns ++ factorReturns    \n",
    "    \n",
    "}\n",
    "\n",
    "def linearModel(instrument: Array[Double], factorMatrix: Array[Array[Double]]): OLSMultipleLinearRegression = {\n",
    "    val regression = new OLSMultipleLinearRegression()\n",
    "    regression.newSampleData(instrument, factorMatrix)\n",
    "    regression\n",
    "}\n",
    "\n",
    "val factorMat = factorMatrix(factors)\n",
    "val factorFeatures = factorMat.map(featurize)\n",
    "val factorWeights = stocks.\n",
    "    map(linearModel(_, factorFeatures)).\n",
    "    map(_.estimateRegressionParameters).\n",
    "    filter( weight => weight(0).isNaN == false).\n",
    "    toArray\n",
    "factorWeights.size // NaN 이 섞인 모델링은 제거\n",
    "stocks.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 표본 추출\n",
    "\n",
    "* 시장 요인 메트릭스에 대한 커널 밀도 추정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factorCov: Array[Array[Double]] = Array(Array(9.3252388430769E-4, 5.9696785542521946E-5, 4.693902205558582E-5), Array(5.9696785542521946E-5, 5.308496396296666E-4, 1.0772004530333134E-4), Array(4.693902205558582E-5, 1.0772004530333134E-4, 5.711355671718794E-5))\n",
      "factorMeans: Array[Double] = Array(0.0011934781923782765, 7.955887959746435E-4, 3.958487265604749E-4)\n",
      "factorDist: org.apache.commons.math3.distribution.MultivariateNormalDistribution = org.apache.commons.math3.distribution.MultivariateNormalDistribution@50f6199f\n",
      "res18: Array[Double] = Array(-0.03410139233221149, 0.052341693200974794, -0.0014622032611104714)\n",
      "res19: Array[Double] = Array(0.03554517817059169, 0.004198462033487468, 0.007745118486443139)\n"
     ]
    }
   ],
   "source": [
    "// 다변량 정규 분포 fitting\n",
    "val factorCov = new Covariance(factorMat).getCovarianceMatrix().getData()   // factorMat : N*3 => cov => 3x3\n",
    "val factorMeans = factors.map( f => f.sum / f.size ).toArray\n",
    "val factorDist = new MultivariateNormalDistribution(factorMeans, factorCov)\n",
    "factorDist.sample()\n",
    "factorDist.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실험 실행하기\n",
    "\n",
    "* 각 trial 마다 X 발생 (from factorDist)\n",
    "* factor weight를 이용해 prediction : y\n",
    "* 손실 계산 \n",
    "* 손실 분포 \n",
    "* 손실 분포로부터 VaR 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallelism: Int = 10\n",
      "baseSeed: Int = 1496\n",
      "seeds: scala.collection.immutable.Range = Range(1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505)\n",
      "seedDS: org.apache.spark.sql.Dataset[Int] = [value: int]\n"
     ]
    }
   ],
   "source": [
    "// 동시 실험 개수 및 난수 발생으로 랜덤성 형성\n",
    "val parallelism = 10\n",
    "val baseSeed = 1496\n",
    "val seeds = (baseSeed until baseSeed+parallelism)\n",
    "\n",
    "//  동시 실험 개수 만큼 리파티션 : Why? seed가 겹치지 않게 해서 실험 독립을 100% 보장하기 위해\n",
    "val seedDS = seeds.toDS().repartition(parallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumentTrialReturn: (instrument: Array[Double], trial: Array[Double])Double\n",
      "trialReturn: (trial: Array[Double], instruments: Seq[Array[Double]])Double\n",
      "trialReturns: (seed: Long, numTrials: Int, instruments: Seq[Array[Double]], factorMeans: Array[Double], factorCov: Array[Array[Double]])Seq[Double]\n",
      "numTrials: Int = 100000\n",
      "trials: scala.collection.immutable.IndexedSeq[Double] = Vector(0.04976403826352305, 0.013224905111413231, 0.01897621299663793, 0.027623531244437113, 0.0026007128847401716, 0.0225436478640878, 0.06048547791773244, 0.040546359361759016, 0.02738448199064011, 0.01144572655993988, 0.04467572063597882, 0.017646599629715872, 0.0022815165918509146, 0.03901623218222861, 0.021567813975053265, 0.011557712834543198, 0.02362046533281404, 0.03453827677770612, 0.010583873501656132, 0.008307565581656174, 0.014592237545986446, 0.012243887917473146, 0.0015932236783471588, 0.03685604323863246, -0.002769623873547922, 0.031094936857761202, -0.036667952517917345, 0.01279695990127956, 0.01120364063761335, 0.03938642509806548, 0.045920038369761516, 0.05906056183181858, -0.0034564384486350385, 0.031104150262721...trialsDS: org.apache.spark.sql.Dataset[Double] = [value: double]\n"
     ]
    }
   ],
   "source": [
    "// instrument : biase + weight arr\n",
    "// ret : wx + b\n",
    "def instrumentTrialReturn(instrument: Array[Double], trial: Array[Double]): Double = {\n",
    "  \n",
    "    // WX + b 계산 \n",
    "    var ret = instrument(0)\n",
    "    var i = 0\n",
    "    while( i < trial.length) {\n",
    "        ret += trial(i)*instrument(i+1)\n",
    "        i += 1\n",
    "    }\n",
    "    \n",
    "    ret\n",
    "}\n",
    "\n",
    "// 모든 금융상품을 고려한 전체 return \n",
    "def trialReturn(trial: Array[Double], instruments:Seq[Array[Double]]): Double = {\n",
    "    instruments.map( instrumentTrialReturn(_, trial) ).sum / instruments.size\n",
    "}\n",
    "\n",
    "// 각 실험에서 모든 금융상품을 고려한 전체 return\n",
    "//  ret = 각 실험별 리턴\n",
    "def trialReturns(seed:Long, numTrials:Int, instruments:Seq[Array[Double]], \n",
    "                 factorMeans: Array[Double], factorCov:Array[Array[Double]]): Seq[Double] = {\n",
    "    \n",
    "    // 다변량 분포 fitting : 각 spark job 마다 다시 특정 seed로부터 순환 주기가 긴 seed generator(MT)를 사용해 실험 독립 보장\n",
    "    val rand = new MersenneTwister(seed)\n",
    "    val factorDist = new MultivariateNormalDistribution(rand, factorMeans, factorCov)\n",
    "    \n",
    "    val trialReturns = new Array[Double](numTrials)\n",
    "\n",
    "    for (i <- 0 until numTrials) {\n",
    "        val sample = factorDist.sample()\n",
    "        val feature = featurize(sample)\n",
    "        trialReturns(i) = trialReturn(feature, instruments)\n",
    "    }\n",
    "    trialReturns\n",
    "}\n",
    "\n",
    "\n",
    "val numTrials = 100000\n",
    "val trials = seeds.flatMap(trialReturns(_, numTrials / parallelism, factorWeights, factorMeans, factorCov))\n",
    "val trialsDS = trials.toDS().cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험 결과의 (정규) 분포 에서 5% quantile 이하값을 VaR 로 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fivePercentVaR: (trials: org.apache.spark.sql.Dataset[Double])Double\n",
      "valueAtRisk: Double = -0.021429430793046003\n"
     ]
    }
   ],
   "source": [
    "def fivePercentVaR(trials:Dataset[Double]): Double = {\n",
    "    trials.stat.approxQuantile(\"value\", Array(0.05), 0.0).head\n",
    "}\n",
    "\n",
    "val valueAtRisk = fivePercentVaR(trialsDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional VaR 은 하위 5% 들의 평균으로 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fivePercentCVaR: (trials: org.apache.spark.sql.Dataset[Double])Double\n",
      "conValueAtRisk: Double = -0.03189059064494651\n"
     ]
    }
   ],
   "source": [
    "def fivePercentCVaR(trials:Dataset[Double]): Double = {\n",
    "    val topLosses = trials.orderBy(\"value\").limit(math.max(trials.count.toInt/20, 1))\n",
    "    topLosses.agg(\"value\" -> \"avg\").first()(0).asInstanceOf[Double]\n",
    "}\n",
    "\n",
    "val conValueAtRisk = fivePercentCVaR(trialsDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 통계량(VaR)의 신뢰구간 측정 (Bootstrap Interval)\n",
    "\n",
    "* bootstrap distribution\n",
    "  * sample로부터 복원추출(sample with replacement, 동일크기)해서 얻은 복수 개의 resample들 확보\n",
    "  * resample로부터의 통계(ex. VaR) 추출 \n",
    "* bootstrap interval\n",
    "  * 얻은 추출치들에서 interval 계산  (ex. 5% 이면 양쪽 2.5% 제외한 구간)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapInteval: (trials: org.apache.spark.sql.Dataset[Double], computeStatistics: org.apache.spark.sql.Dataset[Double] => Double, numResamples: Int, prob: Double)(Double, Double)\n",
      "res65: (Double, Double) = (-0.02162347815202497,-0.02144372937173182)\n",
      "res66: (Double, Double) = (-0.031554605723052076,-0.03177225802643956)\n"
     ]
    }
   ],
   "source": [
    "def bootstrapInteval(\n",
    "    trials: Dataset[Double],\n",
    "    computeStatistics: Dataset[Double] => Double,\n",
    "    numResamples: Int, \n",
    "    prob: Double\n",
    "): (Double, Double) = {\n",
    "    \n",
    "    val stats = (0 until numResamples).map { i =>\n",
    "        val resample = trials.sample(true, 1.0)  // 복원 전수 추출\n",
    "        computeStatistics(resample)\n",
    "    }\n",
    "    \n",
    "    val lowerIndex = (numResamples * prob / 2 - 1).toInt\n",
    "    val upperIndex = math.ceil(numResamples * (1 - prob/2)).toInt\n",
    "    \n",
    "    ( stats(lowerIndex), stats(upperIndex) )\n",
    "}\n",
    "\n",
    "bootstrapInteval(trialsDS, fivePercentVaR, 100, 0.05)\n",
    "bootstrapInteval(trialsDS, fivePercentCVaR, 100, 0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
