{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 스파크 고급 분석 7장\n",
    "\n",
    "## 그래프X로 동시발생 네트워크 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'medline-graphX', 'proxyUser': 'hduser', 'driverMemory': '4000M', 'conf': {'spark.jars.packages': 'graphframes:graphframes:0.3.0-spark2.0-s_2.11', 'spark.master': 'local[2]', 'spark.jars': 'hdfs://localhost:54310/jars/ch06-lsa-2.0.0-jar-with-dependencies.jar', 'spark.sql.crossJoin.enabled': 'true'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"name\": \"medline-graphX\",\n",
    "    \"proxyUser\": \"hduser\",\n",
    "    \"driverMemory\": \"4000M\", \n",
    "    \"conf\": {\"spark.jars.packages\": \"graphframes:graphframes:0.3.0-spark2.0-s_2.11\",\n",
    "             \"spark.master\": \"local[2]\",\n",
    "             \"spark.jars\": \"hdfs://localhost:54310/jars/ch06-lsa-2.0.0-jar-with-dependencies.jar\",\n",
    "             \"spark.sql.crossJoin.enabled\": \"true\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import edu.umd.cloud9.collection.XMLInputFormat\n",
      "import java.nio.charset.StandardCharsets\n",
      "import java.security.MessageDigest\n",
      "import org.apache.hadoop.io.{Text, LongWritable}\n",
      "import org.apache.hadoop.conf.Configuration\n",
      "import org.apache.spark.graphx._\n",
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.sql.{Dataset, DataFrame, SparkSession, Row}\n",
      "import org.apache.spark.sql.functions._\n",
      "import scala.xml.XML\n"
     ]
    }
   ],
   "source": [
    "import edu.umd.cloud9.collection.XMLInputFormat\n",
    "\n",
    "import java.nio.charset.StandardCharsets\n",
    "import java.security.MessageDigest\n",
    "\n",
    "import org.apache.hadoop.io.{Text, LongWritable}\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{Dataset, DataFrame, SparkSession, Row}\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "import scala.xml.XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base: String = hdfs://localhost:54310/medline/\n"
     ]
    }
   ],
   "source": [
    "val base = \"hdfs://localhost:54310/medline/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadMedline: (spark: org.apache.spark.sql.SparkSession, path: String)org.apache.spark.sql.Dataset[String]\n",
      "path: String = hdfs://localhost:54310/medline/medsamp2016a.xml\n",
      "medlineRaw: org.apache.spark.sql.Dataset[String] = [value: string]\n",
      "res6: Long = 30000\n"
     ]
    }
   ],
   "source": [
    "def loadMedline(spark: SparkSession, path:String) = {\n",
    "    \n",
    "    @transient val conf = new Configuration()\n",
    "    conf.set(XMLInputFormat.START_TAG_KEY, \"<MedlineCitation \")\n",
    "    conf.set(XMLInputFormat.END_TAG_KEY, \"</MedlineCitation>\")\n",
    "    val kvs = spark.sparkContext.newAPIHadoopFile(path, classOf[XMLInputFormat], classOf[LongWritable], classOf[Text], conf)\n",
    "    kvs.map(_._2.toString).toDS()\n",
    "}\n",
    "\n",
    "val path = base + \"medsamp2016a.xml\"\n",
    "val medlineRaw = loadMedline(spark, path)\n",
    "medlineRaw.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majorTopics: (record: String)Seq[String]\n",
      "medline: org.apache.spark.sql.Dataset[Seq[String]] = [value: array<string>]\n",
      "res12: medline.type = [value: array<string>]\n",
      "res13: Seq[String] = List(Intellectual Disability, Maternal-Fetal Exchange, Pregnancy Complications)\n"
     ]
    }
   ],
   "source": [
    "def majorTopics(record:String): Seq[String] = {\n",
    "    val elem = XML.loadString(record)\n",
    "    \n",
    "    val dn = elem \\\\ \"DescriptorName\"\n",
    "    \n",
    "    dn.filter( n => (n \\ \"@MajorTopicYN\").text == \"Y\").map( n => n.text)\n",
    "}\n",
    "\n",
    "val medline = medlineRaw.map(majorTopics)\n",
    "medline.cache()\n",
    "medline.take(1)(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동시 발생 조사하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topics: org.apache.spark.sql.DataFrame = [topic: string]\n",
      "topicDist: org.apache.spark.sql.DataFrame = [topic: string, cnt: bigint]\n",
      "res17: Long = 82195\n",
      "+------------+----+\n",
      "|       topic| cnt|\n",
      "+------------+----+\n",
      "|     Disease|1202|\n",
      "|   Neoplasms| 983|\n",
      "|Tuberculosis| 950|\n",
      "|       Blood| 518|\n",
      "|  Anesthesia| 379|\n",
      "+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val topics = medline.flatMap(n => n).toDF(\"topic\")\n",
    "topics.createOrReplaceTempView(\"topics\")\n",
    "\n",
    "val topicDist = spark.sql(\"\"\"\n",
    "SELECT topic, COUNT(*) as cnt\n",
    "FROM topics\n",
    "GROUP BY topic\n",
    "ORDER BY cnt DESC\n",
    "\"\"\")\n",
    "\n",
    "topics.count()\n",
    "topicDist.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|cnt|dist|\n",
      "+---+----+\n",
      "|  1|2347|\n",
      "|  2|1136|\n",
      "|  3| 724|\n",
      "|  4| 487|\n",
      "|  5| 379|\n",
      "|  6| 307|\n",
      "|  7| 199|\n",
      "|  8| 193|\n",
      "|  9| 169|\n",
      "| 10| 141|\n",
      "| 11| 123|\n",
      "| 12| 102|\n",
      "| 15|  89|\n",
      "| 13|  84|\n",
      "| 14|  73|\n",
      "| 16|  63|\n",
      "| 18|  58|\n",
      "| 17|  51|\n",
      "| 20|  50|\n",
      "| 22|  42|\n",
      "+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topicDist.createOrReplaceTempView(\"topic_dist\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT cnt, COUNT(*) as dist\n",
    "FROM topic_dist\n",
    "GROUP BY cnt\n",
    "ORDER BY dist DESC\n",
    "\"\"\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 동시 발생쌍 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getCooccur: (ds: org.apache.spark.sql.Dataset[Seq[String]])org.apache.spark.sql.DataFrame\n",
      "cooccurs: org.apache.spark.sql.DataFrame = [pair: array<string>, cnt: bigint]\n",
      "res22: cooccurs.type = [pair: array<string>, cnt: bigint]\n",
      "+--------------------------------------------------+---+\n",
      "|pair                                              |cnt|\n",
      "+--------------------------------------------------+---+\n",
      "|[Antibiotics, Antitubercular, Dermatologic Agents]|195|\n",
      "|[Analgesia, Anesthesia]                           |181|\n",
      "|[Analgesia, Anesthesia and Analgesia]             |179|\n",
      "|[Anesthesia, Anesthesia and Analgesia]            |177|\n",
      "|[Anesthesia, Anesthesiology]                      |153|\n",
      "+--------------------------------------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getCooccur(ds:Dataset[Seq[String]]) = {\n",
    "    \n",
    "    import spark.implicits._\n",
    "    \n",
    "    val pairs = ds.flatMap{ list => list.combinations(2)}.toDF(\"pair\")\n",
    "    \n",
    "    pairs.createOrReplaceTempView(\"pairs_\")\n",
    "    spark.sql(\"\"\"\n",
    "    SELECT pair, COUNT(*) as cnt\n",
    "    FROM pairs_\n",
    "    GROUP BY pair\n",
    "    ORDER BY cnt DESC\n",
    "    \"\"\")\n",
    "}\n",
    "\n",
    "val cooccurs = getCooccur(medline)\n",
    "cooccurs.cache()\n",
    "cooccurs.show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프X로 동시발생 네트워크 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashId: (str: String)Long\n",
      "vertices: org.apache.spark.sql.DataFrame = [hash: bigint, topic: string]\n",
      "uniqueHashes: Array[org.apache.spark.sql.Row] = Array([7699])\n",
      "<console>:59: warning: non-variable type argument String in type pattern Seq[String] (the underlying of Seq[String]) is unchecked since it is eliminated by erasure\n",
      "       val edges = cooccurs.map { case Row(topics:Seq[String], cnt:Long) =>\n",
      "                                                  ^\n",
      "edges: org.apache.spark.sql.Dataset[org.apache.spark.graphx.Edge[Long]] = [srcId: bigint, dstId: bigint ... 1 more field]\n",
      "vertexRDD: org.apache.spark.rdd.RDD[(Long, String)] = MapPartitionsRDD[99] at map at <console>:54\n",
      "topicGraph: org.apache.spark.graphx.Graph[String,Long] = org.apache.spark.graphx.impl.GraphImpl@59affc97\n",
      "res31: org.apache.spark.graphx.Graph[String,Long] = org.apache.spark.graphx.impl.GraphImpl@59affc97\n"
     ]
    }
   ],
   "source": [
    "// Vertext ID unique하게 결정하기 (주어진 cooccur pair를 hash떠서)\n",
    "def hashId(str: String): Long = {\n",
    "    // This is effectively the same implementation as in Guava's Hashing, but 'inlined'\n",
    "    // to avoid a dependency on Guava just for this. It creates a long from the first 8 bytes\n",
    "    // of the (16 byte) MD5 hash, with first byte as least-significant byte in the long.\n",
    "    val bytes = MessageDigest.getInstance(\"MD5\").digest(str.getBytes(StandardCharsets.UTF_8))\n",
    "    (bytes(0) & 0xFFL) |\n",
    "    ((bytes(1) & 0xFFL) << 8) |\n",
    "    ((bytes(2) & 0xFFL) << 16) |\n",
    "    ((bytes(3) & 0xFFL) << 24) |\n",
    "    ((bytes(4) & 0xFFL) << 32) |\n",
    "    ((bytes(5) & 0xFFL) << 40) |\n",
    "    ((bytes(6) & 0xFFL) << 48) |\n",
    "    ((bytes(7) & 0xFFL) << 56)\n",
    "}\n",
    "\n",
    "//  topic list로부터 vectex 확보\n",
    "val vertices = topics.map{ case Row(topic:String) => (hashId(topic), topic) } toDF(\"hash\", \"topic\")\n",
    "val uniqueHashes = vertices.agg(countDistinct(\"hash\")).take(1)\n",
    "\n",
    "// coocccurs로부터 edge 확보\n",
    "val edges = cooccurs.map { case Row(topics:Seq[String], cnt:Long) =>\n",
    "    val ids = topics.map(_.toString).map(hashId).sorted\n",
    "    Edge(ids(0), ids(1), cnt)\n",
    "}\n",
    "\n",
    "val vertexRDD = vertices.rdd.map{ case Row(hash:Long, topic:String) => (hash, topic)}\n",
    "val topicGraph = Graph(vertexRDD, edges.rdd)\n",
    "topicGraph.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res32: Long = 82195\n"
     ]
    }
   ],
   "source": [
    "vertexRDD.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res33: Long = 7699\n"
     ]
    }
   ],
   "source": [
    "topicGraph.vertices.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res34: Long = 72560\n"
     ]
    }
   ],
   "source": [
    "edges.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res35: org.apache.spark.graphx.VertexRDD[String] = VertexRDDImpl[116] at RDD at VertexRDD.scala:57\n"
     ]
    }
   ],
   "source": [
    "topicGraph.vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 - Connected Components\n",
    "\n",
    "원리 - 이웃에게 자신이 아닌 min vertex id 전달, 같은 min value를 공유하는 것끼리 CC 이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccGraph: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,Long] = org.apache.spark.graphx.impl.GraphImpl@6d86a336\n",
      "ccDF: org.apache.spark.sql.DataFrame = [vid: bigint, cid: bigint]\n",
      "+--------------------+----+\n",
      "|                 cid| cnt|\n",
      "+--------------------+----+\n",
      "|-9215470674759766104|7572|\n",
      "| 1765411469112156596|   3|\n",
      "| 3608770526546285755|   3|\n",
      "|  731949936574312042|   2|\n",
      "|-4492732731552733030|   2|\n",
      "+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val ccGraph = topicGraph.connectedComponents()\n",
    "val ccDF = ccGraph.vertices.toDF(\"vid\", \"cid\")\n",
    "ccDF.createOrReplaceTempView(\"cc\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT cid, COUNT(*) as cnt\n",
    "FROM cc\n",
    "GROUP BY cid\n",
    "ORDER BY cnt DESC\n",
    "LIMIT 5\n",
    "\"\"\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topicCCDF: org.apache.spark.sql.DataFrame = [topic: string, cid: bigint]\n",
      "+--------------------+--------------------+\n",
      "|               topic|                 cid|\n",
      "+--------------------+--------------------+\n",
      "|Spinal Cord Injuries|-9215470674759766104|\n",
      "|              Pyuria|-9215470674759766104|\n",
      "|         Hepatitis A|-9215470674759766104|\n",
      "|     Uveal Neoplasms|-9215470674759766104|\n",
      "|                Skin|-9215470674759766104|\n",
      "|     Radiodermatitis|-9215470674759766104|\n",
      "|   Hand Disinfection|-9215470674759766104|\n",
      "|Forced Expiratory...|-9215470674759766104|\n",
      "|    Hemifacial Spasm|-9215470674759766104|\n",
      "|Benzophenanthridines|-9215470674759766104|\n",
      "|      Duodenal Ulcer|-9215470674759766104|\n",
      "|   Spinal Dysraphism|-9215470674759766104|\n",
      "|        Polymyositis|-9215470674759766104|\n",
      "|             Ketosis|-9215470674759766104|\n",
      "|         Naturopathy|-9215470674759766104|\n",
      "|  Social Environment|-9215470674759766104|\n",
      "|             Syringa|-9215470674759766104|\n",
      "|Unconscious (Psyc...|-9215470674759766104|\n",
      "|     Calcium Sulfate|-9215470674759766104|\n",
      "|       Pharyngectomy|-9215470674759766104|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// def innerJoin[U, VD2](other: RDD[(VertexId, U)])(f: (VertexId, VD, U) => VD2): VertexRDD[VD2]\n",
    "\n",
    "val topicCCDF = topicGraph.vertices.innerJoin(ccGraph.vertices) {\n",
    "    case (topicId, name, cid) => (name, cid.toLong)\n",
    "}.values.toDF(\"topic\", \"cid\")\n",
    "\n",
    "topicCCDF.where($\"cid\" === \"-9215470674759766104\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 - Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degrees: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[257] at RDD at VertexRDD.scala:57\n",
      "res42: org.apache.spark.util.StatCounter = (count: 7596, mean: 19.104792, stdev: 40.490526, max: 1482.000000, min: 1.000000)\n"
     ]
    }
   ],
   "source": [
    "val degrees:VertexRDD[Int] = topicGraph.degrees.cache()\n",
    "degrees.map(_._2).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namesAndDegree: org.apache.spark.sql.DataFrame = [topic: string, degree: int]\n",
      "+-------------------+------+\n",
      "|              topic|degree|\n",
      "+-------------------+------+\n",
      "|            Disease|  1482|\n",
      "|          Neoplasms|   918|\n",
      "|              Blood|   706|\n",
      "|       Tuberculosis|   665|\n",
      "|Wounds and Injuries|   449|\n",
      "+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val namesAndDegree = degrees.innerJoin(topicGraph.vertices) {\n",
    "    (topicId, degree, name) => (name, degree.toInt)\n",
    "}.values.toDF(\"topic\", \"degree\")\n",
    "namesAndDegree.orderBy($\"degree\".desc).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카이제곱 통계량으로 관련이 낮은 관계 필터링하기\n",
    "\n",
    "* 특정 두 주제(A, B)에 대해서 카이제곱 통계량 분석\n",
    "\n",
    "|  | A가 나왔음 | B가 나오지 않음 | A 총계 |\n",
    "|---|:---:|---:|---:|\n",
    "| B가 나왔음 |YY | YN | YA |\n",
    "| B가 나오지 않음 | NY | NN | NA |\n",
    "| B 통계 | YB | NB | T |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T: Long = 30000\n",
      "topicDistRdd: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[643] at rdd at <console>:59\n",
      "topicDistGraph: org.apache.spark.graphx.Graph[Long,Long] = org.apache.spark.graphx.impl.GraphImpl@21c50ba9\n"
     ]
    }
   ],
   "source": [
    "// 전체 문서 개수 T\n",
    "val T = medline.count()\n",
    "\n",
    "// 특정 주제가 등장하는 문서의 수 ( YA, YB 에 해당)\n",
    "val topicDistRdd = topicDist.map {\n",
    "    case Row(topic:String, cnt:Long) => (hashId(topic), cnt)\n",
    "}.rdd\n",
    "val topicDistGraph = Graph(topicDistRdd, topicGraph.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chiSq: (YY: Long, YB: Long, YA: Long, T: Long)Double\n",
      "chiSquaredGraph: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@2b8fb024\n",
      "res148: org.apache.spark.util.StatCounter = (count: 72560, mean: 315.119299, stdev: 1354.301605, max: 29096.810219, min: 0.000000)\n",
      "res149: Long = 72560\n",
      "interesting: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@518c838e\n",
      "res152: Long = 36938\n"
     ]
    }
   ],
   "source": [
    "// 카이제곱 검증값 (X_2)\n",
    "def chiSq(YY:Long, YB:Long, YA:Long, T:Long) = {\n",
    "    val NB = T - YB\n",
    "    val NA = T - YA\n",
    "    val YN = YA - YY\n",
    "    val NY = YB - YY\n",
    "    val NN = T - NY - YN - YY\n",
    "    \n",
    "    val inner = math.abs(YY*NN - YN *NY) -T/2.0\n",
    "    T * math.pow(inner,2) / (YA*NA*YB*NB)\n",
    "}\n",
    "\n",
    "val chiSquaredGraph = topicDistGraph.mapTriplets(triplet => {\n",
    "    // EDGE 속성 : topicGraph에서 왓으므로 co-occur count\n",
    "    // VERTEX 속성 : topicDistGraph에서 왔으므로, 각 토픽의 문서 무관 발생량 (즉 YA or YB)\n",
    "    chiSq(triplet.attr, triplet.srcAttr, triplet.dstAttr, T)\n",
    "    \n",
    "    // 결론적으로 만들어지는 triplet의 edge에 X^2 검증값이 들어있을 것이다. \n",
    "})\n",
    "\n",
    "chiSquaredGraph.edges.map(e=>e.attr).stats()\n",
    "chiSquaredGraph.edges.count\n",
    "\n",
    "// 자유도 1, 99.999%의 기각범위는 19.5, 이것보다 큰 pair는 서로 독립이 아닌 (즉 연관이 깊은 것) 것으로 간주해서 살린다. \n",
    "val interesting = chiSquaredGraph.subgraph( triplet => triplet.attr > 19.5 )\n",
    "interesting.edges.count  // 7만 => 3만여개 정도로 줄였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interestingDegrees: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[704] at RDD at VertexRDD.scala:57\n",
      "res155: org.apache.spark.util.StatCounter = (count: 7587, mean: 9.737182, stdev: 10.634602, max: 247.000000, min: 1.000000)\n"
     ]
    }
   ],
   "source": [
    "val interestingDegrees = interesting.degrees.cache()\n",
    "interestingDegrees.map(_._2).stats()  // 필터링 전과 비교하면 많이 connectivity가 sparse해졌다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Word 이론 검증\n",
    "\n",
    "* 노드 대부분 차수(degree)가 작고, 밀도가 높은 군집에 속해있다. (군집계수가 크다)\n",
    "* 한 노드에서 다른 노드로 빠르게 도달 (small shortest path)\n",
    "\n",
    "* 각 Vertext에서의 군집계수 (k:이웃수, t:트라이앵글수)\n",
    " $$C=\\frac{2t}{k(k-1)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triangleGraph: org.apache.spark.graphx.Graph[Int,Double] = org.apache.spark.graphx.impl.GraphImpl@5cf9f4c5\n",
      "res168: org.apache.spark.util.StatCounter = (count: 7699, mean: 12.089622, stdev: 20.924037, max: 691.000000, min: 0.000000)\n",
      "maxTriGraph: org.apache.spark.graphx.VertexRDD[Double] = VertexRDDImpl[971] at RDD at VertexRDD.scala:57\n",
      "localClusterCoef: org.apache.spark.graphx.VertexRDD[Double] = VertexRDDImpl[973] at RDD at VertexRDD.scala:57\n",
      "res175: Double = 0.35686064879743434\n"
     ]
    }
   ],
   "source": [
    "// 각 꼭지점별 트라이앵글수 구한 후 통계\n",
    "val triangleGraph = interesting.triangleCount()\n",
    "triangleGraph.vertices.map(_._2).stats()\n",
    "\n",
    "// 전체 가능한 트라이앵글 수 = k(k-1)/2\n",
    "val maxTriGraph = interestingDegrees.mapValues( d => d*(d-1)/2.0 )\n",
    "\n",
    "// 지역 군집 계수 \n",
    "val localClusterCoef = triangleGraph.vertices.innerJoin(maxTriGraph) {\n",
    "    (vid, triangleCout, maxTriangleCount) => {\n",
    "        if (maxTriangleCount == 0) 0 else 1.0 * triangleCout / maxTriangleCount\n",
    "    }\n",
    "}\n",
    "\n",
    "// 네트워크 전체에 대한 평균적 지역 군집 계수 (avg)\n",
    "localClusterCoef.map(_._2).sum() / interesting.vertices.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프레겔(대량 동기 병렬 그래프 처리)을 사용한 평균 경로 길이 계산하기\n",
    "\n",
    "프레겔 사용법\n",
    "1) 각 Vertex의 상태 정의 : 자신이 알고있는 경로 길이 정보 = Map[VertextID, Int]\n",
    "\n",
    "2) 이웃으로부터의 메세지와 현재 상태를 종합하여 내보낼 메세지 갱신 함수 : 누가 더 작은 경로 길이를 알면 이걸로 replace해야함\n",
    "\n",
    "3) 자신의 상태 갱신 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "code_folding": [
     1,
     16,
     25,
     38
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mergeMaps: (m1: Map[org.apache.spark.graphx.VertexId,Int], m2: Map[org.apache.spark.graphx.VertexId,Int])Map[org.apache.spark.graphx.VertexId,Int]\n",
      "update: (id: org.apache.spark.graphx.VertexId, state: Map[org.apache.spark.graphx.VertexId,Int], msg: Map[org.apache.spark.graphx.VertexId,Int])Map[org.apache.spark.graphx.VertexId,Int]\n",
      "checkIncrement: (a: Map[org.apache.spark.graphx.VertexId,Int], b: Map[org.apache.spark.graphx.VertexId,Int], bid: org.apache.spark.graphx.VertexId)Iterator[(org.apache.spark.graphx.VertexId, scala.collection.immutable.Map[org.apache.spark.graphx.VertexId,Int])]\n",
      "iterate: (e: org.apache.spark.graphx.EdgeTriplet[Map[org.apache.spark.graphx.VertexId,Int], _])Iterator[(org.apache.spark.graphx.VertexId, scala.collection.immutable.Map[org.apache.spark.graphx.VertexId,Int])]\n"
     ]
    }
   ],
   "source": [
    "// 두 개의 메세지 병합 함수 : 최소거리이므로 min\n",
    "def mergeMaps(m1:Map[VertexId, Int], m2:Map[VertexId, Int]): Map[VertexId, Int] = {\n",
    "    \n",
    "    def minThatExists(k: VertexId):Int = {\n",
    "        math.min(\n",
    "            m1.getOrElse(k, Int.MaxValue),\n",
    "            m2.getOrElse(k, Int.MaxValue)\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    (m1.keySet ++ m2.keySet).map {\n",
    "        k => (k, minThatExists(k))\n",
    "    }.toMap\n",
    "}\n",
    "\n",
    "// 꼭지점 갱신 함수 : state, msg 병합\n",
    "def update(id:VertexId, state:Map[VertexId, Int], msg:Map[VertexId, Int]) = {\n",
    "    mergeMaps(state, msg)\n",
    "}\n",
    "\n",
    "// 각 꼭지점에 보낼 메세지 \n",
    "//  : EdgeTriplet에서  1) +1 증가 (path상의 경유고려), 2) src <-> dst 사이의 병합정보의 갱신이 있을시만 Iterator exist\n",
    "def checkIncrement(\n",
    "    a: Map[VertexId, Int],\n",
    "    b: Map[VertexId, Int],\n",
    "    bid: VertexId) = {\n",
    "    \n",
    "    // +1 중가 \n",
    "    val aplus = a.map{ case (v,d) => (v, d+1) }\n",
    "    \n",
    "    // 병합 메세지 변화시만 전파 \n",
    "    if ( b != mergeMaps(aplus, b)) {\n",
    "        Iterator((bid, aplus))\n",
    "    } else {\n",
    "        Iterator.empty\n",
    "    }\n",
    "}\n",
    "\n",
    "def iterate(e: EdgeTriplet[Map[VertexId, Int], _]) = {\n",
    "    checkIncrement(e.srcAttr, e.dstAttr, e.dstId) ++ \n",
    "    checkIncrement(e.dstAttr, e.srcAttr, e.dstId)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef156109c7f742248192ba45c77477f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '500' from http://localhost:8998/sessions/0/statements/95 with error payload: \"java.util.concurrent.ExecutionException: org.apache.livy.rsc.rpc.RpcException: java.lang.OutOfMemoryError: GC overhead limit exceeded\\n\"\n"
     ]
    }
   ],
   "source": [
    "// 2% 만 하자\n",
    "val fraction = 0.02 \n",
    "val replacement = false\n",
    "val sample = interesting.vertices.map(v => v._1).sample(replacement, fraction, 1792L)\n",
    "val ids = sample.collect().toSet\n",
    "val mapGraph = interesting.mapVertices((id, v) => {\n",
    "    if (ids.contains(id)) {\n",
    "        Map(id -> 0)\n",
    "    } else {\n",
    "        Map[VertexId, Int]()\n",
    "    }\n",
    "})\n",
    "\n",
    "// 프레겔 시작\n",
    "val start = Map[VertexId, Int]() // 초기 메세지 \n",
    "val res = mapGraph.ops.pregel(start)(update, iterate, mergeMaps)   // (vid, vid, path_length)\n",
    "\n",
    "// 결과\n",
    "val paths = res.vertices.flatMap { case (id, m) =>\n",
    "    m.map { case (k, v) =>\n",
    "        if (id < k) {\n",
    "            (id, k, v)\n",
    "        } else {\n",
    "            (k, id, v)\n",
    "        }\n",
    "    }\n",
    "}.distinct().cache()\n",
    "\n",
    "// OOM 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 0 did not reach idle status in time. Current status is busy.\n"
     ]
    }
   ],
   "source": [
    "paths.map(_._3).filter(_ > 0).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 0 did not reach idle status in time. Current status is busy.\n"
     ]
    }
   ],
   "source": [
    "val hist = paths.map(_._3).countByValue()\n",
    "hist.toSeq.sorted.foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
