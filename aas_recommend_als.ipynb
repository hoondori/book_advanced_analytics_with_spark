{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 스파크 고급 분석 3장\n",
    "\n",
    "## 음악 추천과 Audioscrobber 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'proxyUser': 'hduser', 'executorMemory': '4000M', 'executorCores': 6, 'conf': {'spark.jars.packages': 'graphframes:graphframes:0.3.0-spark2.0-s_2.11', 'spark.sql.crossJoin.enabled': 'true'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"proxyUser\": \"hduser\",\n",
    "    \"executorMemory\": \"4000M\", \n",
    "    \"executorCores\": 6,\n",
    "    \"conf\": {\"spark.jars.packages\": \"graphframes:graphframes:0.3.0-spark2.0-s_2.11\",\n",
    "             \"spark.sql.crossJoin.enabled\": \"true\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import scala.collection.Map\n",
      "import scala.collection.mutable.ArrayBuffer\n",
      "import scala.util.Random\n",
      "import org.apache.spark.broadcast.Broadcast\n",
      "import org.apache.spark.ml.recommendation.{ALS, ALSModel}\n",
      "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
      "import org.apache.spark.sql.functions._\n"
     ]
    }
   ],
   "source": [
    "// Optional, but may help avoid errors due to long lineage\n",
    "//spark.sparkContext.setCheckpointDir(\"hdfs:///tmp/\")\n",
    "\n",
    "import scala.collection.Map\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import scala.util.Random\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.ml.recommendation.{ALS, ALSModel}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base: String = hdfs://localhost:54310/audioscrobbler/\n",
      "rawUserArtistData: org.apache.spark.sql.Dataset[String] = [value: string]\n",
      "rawArtistData: org.apache.spark.sql.Dataset[String] = [value: string]\n",
      "rawArtistAlias: org.apache.spark.sql.Dataset[String] = [value: string]\n"
     ]
    }
   ],
   "source": [
    "val base = \"hdfs://localhost:54310/audioscrobbler/\"\n",
    "val rawUserArtistData = spark.read.textFile(base + \"user_artist_data.txt\")\n",
    "val rawArtistData = spark.read.textFile(base + \"artist_data.txt\")\n",
    "val rawArtistAlias = spark.read.textFile(base + \"artist_alias.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     11,
     29,
     48,
     57
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buildArtistAlias: (rawArtistAlias: org.apache.spark.sql.Dataset[String])scala.collection.Map[Int,Int]\n",
      "buildArtistByID: (rawArtistData: org.apache.spark.sql.Dataset[String])org.apache.spark.sql.DataFrame\n",
      "preparation: (rawUserArtistData: org.apache.spark.sql.Dataset[String], rawArtistData: org.apache.spark.sql.Dataset[String], rawArtistAlias: org.apache.spark.sql.Dataset[String])Unit\n",
      "buildCounts: (rawUserArtistData: org.apache.spark.sql.Dataset[String], bArtistAlias: org.apache.spark.broadcast.Broadcast[scala.collection.Map[Int,Int]])org.apache.spark.sql.DataFrame\n",
      "makeRecommendations: (model: org.apache.spark.ml.recommendation.ALSModel, userID: Int, howMany: Int)org.apache.spark.sql.DataFrame\n"
     ]
    }
   ],
   "source": [
    "def buildArtistAlias(rawArtistAlias: Dataset[String]): Map[Int,Int] = {\n",
    "    rawArtistAlias.flatMap { line =>\n",
    "        val Array(artist, alias) = line.split('\\t')\n",
    "        if (artist.isEmpty) {\n",
    "            None\n",
    "        } else {\n",
    "            Some((artist.toInt, alias.toInt))\n",
    "        }\n",
    "    }.collect().toMap\n",
    "}\n",
    "\n",
    "def buildArtistByID(rawArtistData: Dataset[String]): DataFrame = {\n",
    "    rawArtistData.flatMap { line =>\n",
    "        val (id, name) = line.span(_ != '\\t')\n",
    "        if (name.isEmpty) {\n",
    "            None\n",
    "        } else {\n",
    "            try {\n",
    "                Some((id.toInt, name.trim))\n",
    "            } catch {\n",
    "                case _: NumberFormatException => None\n",
    "            }\n",
    "        }\n",
    "    }.toDF(\"id\", \"name\")\n",
    "}\n",
    "\n",
    "def preparation(\n",
    "    rawUserArtistData: Dataset[String],\n",
    "    rawArtistData: Dataset[String],\n",
    "    rawArtistAlias: Dataset[String]): Unit = {\n",
    "    rawUserArtistData.take(5).foreach(println)\n",
    "\n",
    "    val userArtistDF = rawUserArtistData.map { line =>\n",
    "      val Array(user, artist, _*) = line.split(' ')\n",
    "      (user.toInt, artist.toInt)\n",
    "    }.toDF(\"user\", \"artist\")\n",
    "\n",
    "    userArtistDF.agg(min(\"user\"), max(\"user\"), min(\"artist\"), max(\"artist\")).show()\n",
    "\n",
    "    val artistByID = buildArtistByID(rawArtistData)\n",
    "    val artistAlias = buildArtistAlias(rawArtistAlias)\n",
    "\n",
    "    val (badID, goodID) = artistAlias.head\n",
    "    artistByID.filter($\"id\" isin (badID, goodID)).show()\n",
    "}\n",
    "\n",
    "def buildCounts(\n",
    "    rawUserArtistData: Dataset[String],\n",
    "    bArtistAlias: Broadcast[Map[Int,Int]]): DataFrame = {\n",
    "\n",
    "    rawUserArtistData.map { line =>\n",
    "        val Array(userID, artistID, count) = line.split(' ').map(_.toInt)\n",
    "        val finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)\n",
    "        (userID, finalArtistID, count)\n",
    "    }.toDF(\"user\", \"artist\", \"count\")\n",
    "}\n",
    "\n",
    "def makeRecommendations(model: ALSModel, userID: Int, howMany: Int): DataFrame = {\n",
    "    val toRecommend = model.itemFactors.\n",
    "        select($\"id\".as(\"artist\")).\n",
    "        withColumn(\"user\", lit(userID))\n",
    "    \n",
    "    model.transform(toRecommend).\n",
    "        select(\"artist\", \"prediction\").\n",
    "        orderBy($\"prediction\".desc).\n",
    "        limit(howMany)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000002 1 55\n",
      "1000002 1000006 33\n",
      "1000002 1000007 8\n",
      "1000002 1000009 144\n",
      "1000002 1000010 314\n",
      "+---------+---------+-----------+-----------+\n",
      "|min(user)|max(user)|min(artist)|max(artist)|\n",
      "+---------+---------+-----------+-----------+\n",
      "|       90|  2443548|          1|   10794401|\n",
      "+---------+---------+-----------+-----------+\n",
      "\n",
      "+-------+----------------+\n",
      "|     id|            name|\n",
      "+-------+----------------+\n",
      "|1208690|Collective Souls|\n",
      "|1003926| Collective Soul|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparation(rawUserArtistData, rawArtistData, rawArtistAlias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: (rawUserArtistData: org.apache.spark.sql.Dataset[String], rawArtistData: org.apache.spark.sql.Dataset[String], rawArtistAlias: org.apache.spark.sql.Dataset[String])(org.apache.spark.ml.recommendation.ALSModel, org.apache.spark.sql.DataFrame, org.apache.spark.sql.DataFrame)\n"
     ]
    }
   ],
   "source": [
    "def model(\n",
    "    rawUserArtistData: Dataset[String],\n",
    "    rawArtistData: Dataset[String],\n",
    "    rawArtistAlias: Dataset[String]):(ALSModel, DataFrame, DataFrame) = {\n",
    "\n",
    "    val bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "\n",
    "    val allData = buildCounts(rawUserArtistData, bArtistAlias)\n",
    "    val Array(trainData, testData) = allData.randomSplit(Array(0.9, 0.1))\n",
    "    trainData.cache()\n",
    "    testData.cache()\n",
    "\n",
    "    val model = new ALS().\n",
    "        setSeed(Random.nextLong()).\n",
    "        setImplicitPrefs(true).\n",
    "        setRank(10).\n",
    "        setRegParam(0.01).\n",
    "        setAlpha(1.0).\n",
    "        setMaxIter(5).\n",
    "        setUserCol(\"user\").\n",
    "        setItemCol(\"artist\").\n",
    "        setRatingCol(\"count\").\n",
    "        setPredictionCol(\"prediction\").\n",
    "        fit(trainData)\n",
    "\n",
    "    (model, trainData, testData)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainedModel: org.apache.spark.ml.recommendation.ALSModel = als_a7c6535d9ecf\n",
      "trainData: org.apache.spark.sql.DataFrame = [user: int, artist: int ... 1 more field]\n",
      "testData: org.apache.spark.sql.DataFrame = [user: int, artist: int ... 1 more field]\n",
      "model: org.apache.spark.ml.recommendation.ALSModel = als_a7c6535d9ecf\n"
     ]
    }
   ],
   "source": [
    "// val (trainedModel, trainData, testData) = model(rawUserArtistData, rawArtistData, rawArtistAlias)\n",
    "// trainedModel.save(base + \"saved_als_model\")\n",
    "\n",
    "val model = ALSModel.load(base + \"saved_als_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userID: Int = 2093760\n",
      "existingArtistIDs: Array[Int] = Array(378, 813, 1180, 1255340)\n"
     ]
    }
   ],
   "source": [
    "// val bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "// val trainData = buildCounts(rawUserArtistData, bArtistAlias).cache()\n",
    "val userID = 2093760\n",
    "val existingArtistIDs = trainData.\n",
    "filter($\"user\" === userID).\n",
    "select(\"artist\").as[Int].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artistByID: org.apache.spark.sql.DataFrame = [id: int, name: string]\n",
      "+-------+---------------+\n",
      "|     id|           name|\n",
      "+-------+---------------+\n",
      "|   1180|     David Gray|\n",
      "|    378|  Blackalicious|\n",
      "|    813|     Jurassic 5|\n",
      "|1255340|The Saw Doctors|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val artistByID = buildArtistByID(rawArtistData)\n",
    "artistByID.filter($\"id\" isin (existingArtistIDs:_*)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topRecommendations: org.apache.spark.sql.DataFrame = [artist: int, prediction: float]\n",
      "+-------+-----------+\n",
      "| artist| prediction|\n",
      "+-------+-----------+\n",
      "|1037970|0.013426684|\n",
      "|1007614| 0.01282847|\n",
      "|    250|0.012546132|\n",
      "|1002061|0.012249939|\n",
      "|    813|0.012233459|\n",
      "+-------+-----------+\n",
      "\n",
      "recommendedArtistIDs: Array[Int] = Array(1037970, 1007614, 250, 1002061, 813)\n",
      "+-------+------------+\n",
      "|     id|        name|\n",
      "+-------+------------+\n",
      "|1007614|       Jay-Z|\n",
      "|1037970|  Kanye West|\n",
      "|1002061|Jack Johnson|\n",
      "|    813|  Jurassic 5|\n",
      "|    250|     Outkast|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val topRecommendations = makeRecommendations(model, userID, 5)\n",
    "topRecommendations.show()\n",
    "\n",
    "val recommendedArtistIDs = topRecommendations.select(\"artist\").as[Int].collect()\n",
    "artistByID.filter($\"id\" isin (recommendedArtistIDs:_*)).show()\n",
    "\n",
    "// model.userFactors.unpersist()\n",
    "// model.itemFactors.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderCurve: (positiveData: org.apache.spark.sql.DataFrame, bAllArtistIDs: org.apache.spark.broadcast.Broadcast[Array[Int]], predictFunction: org.apache.spark.sql.DataFrame => org.apache.spark.sql.DataFrame)Double\n"
     ]
    }
   ],
   "source": [
    "def areaUnderCurve(\n",
    "    positiveData: DataFrame,\n",
    "    bAllArtistIDs: Broadcast[Array[Int]],\n",
    "    predictFunction: (DataFrame => DataFrame)): Double = {\n",
    "\n",
    "    // What this actually computes is AUC, per user. The result is actually something\n",
    "    // that might be called \"mean AUC\".\n",
    "\n",
    "    // Take held-out data as the \"positive\".\n",
    "    // Make predictions for each of them, including a numeric score\n",
    "    val positivePredictions = predictFunction(positiveData.select(\"user\", \"artist\")).\n",
    "        withColumnRenamed(\"prediction\", \"positivePrediction\")\n",
    "\n",
    "    // BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of\n",
    "    // small AUC problems, and it would be inefficient, when a direct computation is available.\n",
    "\n",
    "    // Create a set of \"negative\" products for each user. These are randomly chosen\n",
    "    // from among all of the other artists, excluding those that are \"positive\" for the user.\n",
    "    val negativeData = positiveData.select(\"user\", \"artist\").as[(Int,Int)].\n",
    "      groupByKey { case (user, _) => user }.\n",
    "      flatMapGroups { case (userID, userIDAndPosArtistIDs) =>\n",
    "        val random = new Random()\n",
    "        val posItemIDSet = userIDAndPosArtistIDs.map { case (_, artist) => artist }.toSet\n",
    "        val negative = new ArrayBuffer[Int]()\n",
    "        val allArtistIDs = bAllArtistIDs.value\n",
    "        var i = 0\n",
    "        // Make at most one pass over all artists to avoid an infinite loop.\n",
    "        // Also stop when number of negative equals positive set size\n",
    "        while (i < allArtistIDs.length && negative.size < posItemIDSet.size) {\n",
    "          val artistID = allArtistIDs(random.nextInt(allArtistIDs.length))\n",
    "          // Only add new distinct IDs\n",
    "          if (!posItemIDSet.contains(artistID)) {\n",
    "            negative += artistID\n",
    "          }\n",
    "          i += 1\n",
    "        }\n",
    "        // Return the set with user ID added back\n",
    "        negative.map(artistID => (userID, artistID))\n",
    "      }.toDF(\"user\", \"artist\")\n",
    "\n",
    "    // Make predictions on the rest:\n",
    "    val negativePredictions = predictFunction(negativeData).\n",
    "      withColumnRenamed(\"prediction\", \"negativePrediction\")\n",
    "\n",
    "    // Join positive predictions to negative predictions by user, only.\n",
    "    // This will result in a row for every possible pairing of positive and negative\n",
    "    // predictions within each user.\n",
    "    val joinedPredictions = positivePredictions.join(negativePredictions, \"user\").\n",
    "      select(\"user\", \"positivePrediction\", \"negativePrediction\").cache()\n",
    "\n",
    "    // Count the number of pairs per user\n",
    "    val allCounts = joinedPredictions.\n",
    "      groupBy(\"user\").agg(count(lit(\"1\")).as(\"total\")).\n",
    "      select(\"user\", \"total\")\n",
    "    // Count the number of correctly ordered pairs per user\n",
    "    val correctCounts = joinedPredictions.\n",
    "      filter($\"positivePrediction\" > $\"negativePrediction\").\n",
    "      groupBy(\"user\").agg(count(\"user\").as(\"correct\")).\n",
    "      select(\"user\", \"correct\")\n",
    "\n",
    "    // Combine these, compute their ratio, and average over all users\n",
    "    val meanAUC = allCounts.join(correctCounts, Seq(\"user\"), \"left_outer\").\n",
    "      select($\"user\", (coalesce($\"correct\", lit(0)) / $\"total\").as(\"auc\")).\n",
    "      agg(mean(\"auc\")).\n",
    "      as[Double].first()\n",
    "\n",
    "    joinedPredictions.unpersist()\n",
    "\n",
    "    meanAUC\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bArtistAlias: org.apache.spark.broadcast.Broadcast[scala.collection.Map[Int,Int]] = Broadcast(181)\n",
      "allData: org.apache.spark.sql.DataFrame = [user: int, artist: int ... 1 more field]\n",
      "allArtistIDs: Array[Int] = Array(1001129, 1003373, 1007972, 1029443, 1076507, 1318111, 833, 1239413, 1000636, 1002431, 1005697, 1040360, 1043263, 1245208, 463, 1043126, 1001601, 1091589, 1004021, 1012885, 1023660, 1004666, 1004739, 1005158, 1005476, 1009031, 1004552, 1233083, 1007334, 1012609, 1348498, 7014014, 1239554, 1007290, 1007777, 1048379, 1013212, 1247803, 1281854, 2164368, 2366, 1054452, 1087384, 1261793, 4935, 1126737, 6623644, 1008081, 2132425, 1168540, 10699021, 496, 1014191, 1009575, 10725691, 1259455, 2060508, 3175, 6896492, 1012902, 1023841, 1088214, 1008233, 1024037, 10368773, 1062158, 1092115, 1236684, 2136388, 1022960, 1247265, 1006814, 1014690, 1104368, 1237375, 10057880, 1017864, 1017973, 1099354, 1329652, 2014256, 2158098, 6656832, 6696725, 1034510, 1291109, 2146392...bAllArtistIDs: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(185)\n",
      "auc: Double = 0.9040250468149337\n"
     ]
    }
   ],
   "source": [
    "val bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "val allData = buildCounts(rawUserArtistData, bArtistAlias)\n",
    "val allArtistIDs = allData.select(\"artist\").as[Int].distinct().collect()\n",
    "val bAllArtistIDs = spark.sparkContext.broadcast(allArtistIDs)\n",
    "val auc = areaUnderCurve(testData, bAllArtistIDs, model.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
