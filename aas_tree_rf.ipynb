{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 스파크 고급 분석 4장\n",
    "\n",
    "## 의사 결정 나무로 산림 식생 분포 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'proxyUser': 'hduser', 'driverMemory': '4000M', 'conf': {'spark.jars.packages': 'graphframes:graphframes:0.3.0-spark2.0-s_2.11', 'spark.master': 'local[2]', 'spark.sql.crossJoin.enabled': 'true'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"proxyUser\": \"hduser\",\n",
    "    \"driverMemory\": \"4000M\", \n",
    "    \"conf\": {\"spark.jars.packages\": \"graphframes:graphframes:0.3.0-spark2.0-s_2.11\",\n",
    "             \"spark.master\": \"local[2]\",\n",
    "             \"spark.sql.crossJoin.enabled\": \"true\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.spark.ml.feature.VectorAssembler\n",
      "import org.apache.spark.ml.linalg.Vector\n",
      "import org.apache.spark.ml.feature.VectorAssembler\n",
      "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
      "import scala.util.Random\n",
      "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
      "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
      "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
      "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
      "import org.apache.spark.ml.feature.VectorIndexer\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import scala.util.Random\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.VectorIndexer\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base: String = hdfs://localhost:54310/covtype/\n"
     ]
    }
   ],
   "source": [
    "val base = \"hdfs://localhost:54310/covtype/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습/평가 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataWithoutHeader: org.apache.spark.sql.DataFrame = [_c0: int, _c1: int ... 53 more fields]\n",
      "colNames: Seq[String] = List(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, Soil_...data: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 53 more fields]\n",
      "trainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Elevation: int, Aspect: int ... 53 more fields]\n",
      "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Elevation: int, Aspect: int ... 53 more fields]\n",
      "res1: trainData.type = [Elevation: int, Aspect: int ... 53 more fields]\n",
      "res2: testData.type = [Elevation: int, Aspect: int ... 53 more fields]\n"
     ]
    }
   ],
   "source": [
    "val dataWithoutHeader = spark.read.option(\"inferSchema\", \"true\").option(\"header\", false).csv(base + \"covtype.data\")\n",
    "val colNames = Seq(\n",
    "        \"Elevation\", \"Aspect\", \"Slope\",\n",
    "        \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\",\n",
    "        \"Horizontal_Distance_To_Roadways\",\n",
    "        \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n",
    "        \"Horizontal_Distance_To_Fire_Points\"\n",
    "      ) ++ (\n",
    "        (0 until 4).map(i => s\"Wilderness_Area_$i\")\n",
    "      ) ++ (\n",
    "        (0 until 40).map(i => s\"Soil_Type_$i\")\n",
    "      ) ++ Seq(\"Cover_Type\")\n",
    "val data = dataWithoutHeader.toDF(colNames: _*).withColumn(\"Cover_Type\", $\"Cover_Type\".cast(\"double\"))\n",
    "val Array(trainData, testData) = data.randomSplit(Array(0.9, 0.1))\n",
    "trainData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res3: org.apache.spark.sql.Row = [1859,18,12,67,11,90,211,215,139,792,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.0]\n"
     ]
    }
   ],
   "source": [
    "trainData.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.VectorAssembler\n",
      "inputCols: Array[String] = Array(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, S...assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_7822c2cfc3e7\n",
      "assembledTrainData: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 54 more fields]\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|featureVector                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1859.0,18.0,12.0,67.0,11.0,90.0,211.0,215.0,139.0,792.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1860.0,18.0,13.0,95.0,15.0,90.0,210.0,213.0,138.0,780.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1861.0,35.0,14.0,60.0,11.0,85.0,218.0,209.0,124.0,832.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1863.0,37.0,17.0,120.0,18.0,90.0,217.0,202.0,115.0,769.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1868.0,27.0,16.0,67.0,17.0,95.0,212.0,204.0,125.0,859.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1871.0,22.0,22.0,60.0,12.0,85.0,200.0,187.0,115.0,792.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1871.0,36.0,19.0,134.0,26.0,120.0,215.0,194.0,107.0,797.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1871.0,37.0,19.0,120.0,29.0,90.0,216.0,195.0,107.0,759.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1872.0,12.0,27.0,85.0,25.0,60.0,182.0,174.0,118.0,577.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1872.0,27.0,16.0,95.0,22.0,124.0,212.0,205.0,126.0,847.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1872.0,27.0,21.0,108.0,30.0,67.0,206.0,190.0,112.0,713.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1872.0,35.0,21.0,120.0,18.0,85.0,213.0,189.0,104.0,797.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,5,6,7,8,9,13,18],[1874.0,18.0,14.0,90.0,208.0,209.0,135.0,793.0,1.0,1.0])                |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1876.0,25.0,17.0,124.0,26.0,150.0,209.0,200.0,123.0,836.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1876.0,29.0,19.0,124.0,34.0,90.0,210.0,195.0,115.0,750.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1877.0,19.0,18.0,85.0,25.0,108.0,204.0,199.0,127.0,886.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1877.0,27.0,24.0,90.0,18.0,95.0,201.0,179.0,104.0,780.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1877.0,28.0,22.0,127.0,35.0,85.0,205.0,185.0,107.0,706.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,5,6,7,8,9,13,18],[1879.0,18.0,14.0,120.0,208.0,210.0,137.0,767.0,1.0,1.0])               |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1879.0,23.0,18.0,108.0,28.0,134.0,207.0,200.0,124.0,875.0,1.0,1.0])|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val inputCols = trainData.columns.filterNot( _ == \"Cover_Type\")\n",
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(inputCols).\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "val assembledTrainData = assembler.transform(trainData)\n",
    "assembledTrainData.select(\"featureVector\").show(truncate=false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트리 구축 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
      "import scala.util.Random\n",
      "model: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel (uid=dtc_b91359e8f0e8) of depth 5 with 41 nodes\n",
      "DecisionTreeClassificationModel (uid=dtc_b91359e8f0e8) of depth 5 with 41 nodes\n",
      "  If (feature 0 <= 3048.5)\n",
      "   If (feature 0 <= 2476.5)\n",
      "    If (feature 3 <= 15.0)\n",
      "     If (feature 12 <= 0.5)\n",
      "      If (feature 23 <= 0.5)\n",
      "       Predict: 4.0\n",
      "      Else (feature 23 > 0.5)\n",
      "       Predict: 3.0\n",
      "     Else (feature 12 > 0.5)\n",
      "      Predict: 6.0\n",
      "    Else (feature 3 > 15.0)\n",
      "     If (feature 16 <= 0.5)\n",
      "      Predict: 3.0\n",
      "     Else (feature 16 > 0.5)\n",
      "      If (feature 9 <= 1288.5)\n",
      "       Predict: 3.0\n",
      "      Else (feature 9 > 1288.5)\n",
      "       Predict: 4.0\n",
      "   Else (feature 0 > 2476.5)\n",
      "    If (feature 17 <= 0.5)\n",
      "     If (feature 15 <= 0.5)\n",
      "      Predict: 2.0\n",
      "     Else (feature 15 > 0.5)\n",
      "      Predict: 3.0\n",
      "    Else (feature 17 > 0.5)\n",
      "     If (feature 0 <= 2697.5)\n",
      "      Predict: 3.0\n",
      "     Else (feature 0 > 2697.5)\n",
      "      If (feature 5 <= 1223.0)\n",
      "       Predict: 5.0\n",
      "      Else (feature 5 > 1223.0)\n",
      "       Predict: 2.0\n",
      "  Else (feature 0 > 3048.5)\n",
      "   If (feature 0 <= 3312.5)\n",
      "    If (feature 7 <= 239.5)\n",
      "     Predict: 1.0\n",
      "    Else (feature 7 > 239.5)\n",
      "     If (feature 3 <= 333.0)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 333.0)\n",
      "      If (feature 0 <= 3223.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 3223.5)\n",
      "       Predict: 1.0\n",
      "   Else (feature 0 > 3312.5)\n",
      "    If (feature 12 <= 0.5)\n",
      "     If (feature 3 <= 296.0)\n",
      "      If (feature 6 <= 206.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 6 > 206.5)\n",
      "       Predict: 7.0\n",
      "     Else (feature 3 > 296.0)\n",
      "      Predict: 1.0\n",
      "    Else (feature 12 > 0.5)\n",
      "     If (feature 45 <= 0.5)\n",
      "      Predict: 7.0\n",
      "     Else (feature 45 > 0.5)\n",
      "      If (feature 5 <= 917.0)\n",
      "       Predict: 7.0\n",
      "      Else (feature 5 > 917.0)\n",
      "       Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val model = new DecisionTreeClassifier().\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setFeaturesCol(\"featureVector\").\n",
    "    setPredictionCol(\"prediction\").\n",
    "    fit(assembledTrainData)\n",
    "println(model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8243706121415982,Elevation)\n",
      "(0.04250140940349825,Soil_Type_3)\n",
      "(0.03314313258774212,Soil_Type_1)\n",
      "(0.02693294405496738,Hillshade_Noon)\n",
      "(0.024444469339088008,Horizontal_Distance_To_Hydrology)\n",
      "(0.018481412770745796,Soil_Type_31)\n",
      "(0.015692478668951324,Wilderness_Area_2)\n",
      "(0.004601426844001658,Horizontal_Distance_To_Roadways)\n",
      "(0.0037830258112822664,Soil_Type_2)\n",
      "(0.002809083504044355,Hillshade_9am)\n",
      "(0.00227764710278306,Horizontal_Distance_To_Fire_Points)\n",
      "(9.623577712976215E-4,Soil_Type_9)\n",
      "(0.0,Wilderness_Area_3)\n",
      "(0.0,Wilderness_Area_1)\n",
      "(0.0,Wilderness_Area_0)\n",
      "(0.0,Vertical_Distance_To_Hydrology)\n",
      "(0.0,Soil_Type_8)\n",
      "(0.0,Soil_Type_7)\n",
      "(0.0,Soil_Type_6)\n",
      "(0.0,Soil_Type_5)\n",
      "(0.0,Soil_Type_4)\n",
      "(0.0,Soil_Type_39)\n",
      "(0.0,Soil_Type_38)\n",
      "(0.0,Soil_Type_37)\n",
      "(0.0,Soil_Type_36)\n",
      "(0.0,Soil_Type_35)\n",
      "(0.0,Soil_Type_34)\n",
      "(0.0,Soil_Type_33)\n",
      "(0.0,Soil_Type_32)\n",
      "(0.0,Soil_Type_30)\n",
      "(0.0,Soil_Type_29)\n",
      "(0.0,Soil_Type_28)\n",
      "(0.0,Soil_Type_27)\n",
      "(0.0,Soil_Type_26)\n",
      "(0.0,Soil_Type_25)\n",
      "(0.0,Soil_Type_24)\n",
      "(0.0,Soil_Type_23)\n",
      "(0.0,Soil_Type_22)\n",
      "(0.0,Soil_Type_21)\n",
      "(0.0,Soil_Type_20)\n",
      "(0.0,Soil_Type_19)\n",
      "(0.0,Soil_Type_18)\n",
      "(0.0,Soil_Type_17)\n",
      "(0.0,Soil_Type_16)\n",
      "(0.0,Soil_Type_15)\n",
      "(0.0,Soil_Type_14)\n",
      "(0.0,Soil_Type_13)\n",
      "(0.0,Soil_Type_12)\n",
      "(0.0,Soil_Type_11)\n",
      "(0.0,Soil_Type_10)\n",
      "(0.0,Soil_Type_0)\n",
      "(0.0,Slope)\n",
      "(0.0,Hillshade_3pm)\n",
      "(0.0,Aspect)\n"
     ]
    }
   ],
   "source": [
    "model.featureImportances.toArray.zip(inputCols).sorted.reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 57 more fields]\n",
      "+----------+----------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|Cover_Type|prediction|probability                                                                                                        |\n",
      "+----------+----------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|3.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|3.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|3.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|3.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|3.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|6.0       |4.0       |[0.0,0.0,0.023982558139534885,0.2492732558139535,0.6308139534883721,0.0,0.09593023255813954,0.0]                   |\n",
      "|3.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|3.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "|6.0       |4.0       |[0.0,0.0,0.023982558139534885,0.2492732558139535,0.6308139534883721,0.0,0.09593023255813954,0.0]                   |\n",
      "|3.0       |3.0       |[0.0,3.4258307639602604E-5,0.048304213771839674,0.6236382322713258,0.02195957519698527,0.0,0.30606372045220964,0.0]|\n",
      "+----------+----------+-------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// 학습 데이터의 예측\n",
    "val predictions = model.transform(assembledTrainData)\n",
    "predictions.select(\"Cover_Type\", \"prediction\", \"probability\").show(truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
      "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_d5da4cd87dd8\n",
      "res42: Double = 0.7033956516830929\n",
      "res43: Double = 0.6879709790220652\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator().\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setPredictionCol(\"prediction\")\n",
    "\n",
    "evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "evaluator.setMetricName(\"f1\").evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 혼동 행렬 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
      "confusionMatrix: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Cover_Type: double, 1: bigint ... 6 more fields]\n",
      "+----------+------+------+-----+----+---+---+-----+\n",
      "|Cover_Type|1     |2     |3    |4   |5  |6  |7    |\n",
      "+----------+------+------+-----+----+---+---+-----+\n",
      "|1.0       |128661|56251 |83   |0   |34 |3  |5317 |\n",
      "|2.0       |51510 |199538|2785 |33  |358|37 |782  |\n",
      "|3.0       |0     |4665  |27089|357 |35 |93 |0    |\n",
      "|4.0       |0     |12    |1325 |1134|0  |0  |0    |\n",
      "|5.0       |0     |7827  |260  |0   |432|0  |0    |\n",
      "|6.0       |0     |5049  |9948 |132 |11 |401|0    |\n",
      "|7.0       |8031  |79    |0    |0   |0  |0  |10367|\n",
      "+----------+------+------+-----+----+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "\n",
    "// MulticlassMetrics\n",
    "//   param: predictionAndLabels an RDD of (prediction, label, weight, probability) or (prediction, label, weight) or (prediction, label) tuples.\n",
    "\n",
    "// val predictionRDD = predictions.\n",
    "//     select(\"prediction\", \"Cover_Type\").\n",
    "//     as[(Double, Double)].\n",
    "//     rdd\n",
    "\n",
    "// val multiClassMetrics = new MulticlassMetrics(predictionRDD)\n",
    "// multiClassMetrics.confusionMatrix\n",
    "\n",
    "// 직접 구현 (by Pivot)\n",
    "\n",
    "val confusionMatrix = predictions.\n",
    "    groupBy(\"Cover_Type\").\n",
    "    pivot(\"prediction\", (1 to 7)).\n",
    "    count.\n",
    "    na.fill(0.0).\n",
    "    orderBy(\"Cover_Type\")\n",
    "confusionMatrix.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70% 정확도가 괜찮은 것일까? \n",
    "\n",
    "무작위 분류기의 성능보다 좋은지\n",
    "* 학습 class prior * 평가 class prior 가 각 example을 맞출(accurate) 확률 \n",
    "* 이를 누적하여 무작위 분류기의 accuracy 추계\n",
    "\n",
    "결론 : 70% 가 37% 보다는 우수하므로 적어도 무작위 분류기보다는 성능이 우수하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.DataFrame\n",
      "getClassPrior: (df: org.apache.spark.sql.DataFrame)Array[Double]\n",
      "trainDataClassProbs: Array[Double] = Array(0.364207416591567, 0.4879907546126485, 0.06168502541907512, 0.004727928838069872, 0.016299969960144572, 0.029735630138585142, 0.035353274439909765)\n",
      "testDataClassProbs: Array[Double] = Array(0.36816678944032344, 0.4840936734449146, 0.06021619584396896, 0.00472821338632587, 0.01668579651551231, 0.03128158566460521, 0.034827745704349614)\n",
      "res153: Double = 0.3764925349876059\n"
     ]
    }
   ],
   "source": [
    "def getClassPrior(df:DataFrame): Array[Double] = {\n",
    "    val totalCount = df.count\n",
    "    \n",
    "    df.groupBy(\"Cover_Type\").count.\n",
    "       orderBy(\"Cover_Type\").\n",
    "       select(\"count\").as[Double].\n",
    "       map ( _ / totalCount).\n",
    "       collect()\n",
    "}\n",
    "\n",
    "val trainDataClassProbs = getClassPrior(trainData)\n",
    "val testDataClassProbs = getClassPrior(testData)\n",
    "\n",
    "trainDataClassProbs.zip(testDataClassProbs).map { case (trainClassProb, testClassProb) =>\n",
    "        trainClassProb * testClassProb\n",
    "}.sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의사 결정 나무 HPO\n",
    "\n",
    "* 참고 : https://spark.apache.org/docs/latest/ml-tuning.html\n",
    "\n",
    "* By TrainValidationSplit \n",
    "  * estimator(pipeline)\n",
    "  * evaluator\n",
    "  * paramGrid\n",
    "  * fold = 1\n",
    "* pram search \n",
    "  * 불순도 기준, 최대 깊이, 최대Bin개수, 최소 infoGain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.VectorAssembler\n",
      "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
      "import scala.util.Random\n",
      "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
      "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
      "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
      "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
      "inputCols: Array[String] = Array(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, S...assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_061a35de4576\n",
      "classifier: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_624f0acc2372\n",
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_658c638c85f8\n",
      "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_e7b34d93be7d\n",
      "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
      "Array({\n",
      "\tdtc_624f0acc2372-impurity: gini,\n",
      "\tdtc_624f0acc2372-maxBins: 40,\n",
      "\tdtc_624f0acc2372-maxDepth: 1,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.0\n",
      "}, {\n",
      "\tdtc_624f0acc2372-impurity: gini,\n",
      "\tdtc_624f0acc2372-maxBins: 40,\n",
      "\tdtc_624f0acc2372-maxDepth: 1,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.05\n",
      "}, {\n",
      "\tdtc_624f0acc2372-impurity: gini,\n",
      "\tdtc_624f0acc2372-maxBins: 40,\n",
      "\tdtc_624f0acc2372-maxDepth: 20,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.0\n",
      "}, {\n",
      "\tdtc_624f0acc2372-impurity: gini,\n",
      "\tdtc_624f0acc2372-maxBins: 40,\n",
      "\tdtc_624f0acc2372-maxDepth: 20,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.05\n",
      "}, {\n",
      "\tdtc_624f0acc2372-impurity: gini,\n",
      "\tdtc_624f0acc2372-maxBins: 300,\n",
      "\tdtc_624f0acc2372-maxDepth: 1,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.0\n",
      "}, {\n",
      "\tdtc_624f0acc2372-impurity: gini,\n",
      "\tdtc_624f0acc23...validator: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_6364712f7830\n",
      "validatorModel: org.apache.spark.ml.tuning.TrainValidationSplitModel = tvs_6364712f7830\n"
     ]
    }
   ],
   "source": [
    "\n",
    "// 파이프라인 구축: assembler, classifier\n",
    "val inputCols = trainData.columns.filterNot( _ == \"Cover_Type\")\n",
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(inputCols).\n",
    "    setOutputCol(\"featureVector\")\n",
    "val classifier = new DecisionTreeClassifier().\n",
    "    setSeed(Random.nextLong()).\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setFeaturesCol(\"featureVector\").\n",
    "    setPredictionCol(\"prediction\")\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, classifier))\n",
    "\n",
    "// evaluator 준비\n",
    "val evaluator = new MulticlassClassificationEvaluator().\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setPredictionCol(\"prediction\").\n",
    "    setMetricName(\"accuracy\")\n",
    "\n",
    "// ParamGrid 준비\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "    addGrid(classifier.impurity, Seq(\"gini\", \"entropy\")).\n",
    "    addGrid(classifier.maxDepth, Seq(1, 20)).\n",
    "    addGrid(classifier.maxBins, Seq(40, 300)).\n",
    "    addGrid(classifier.minInfoGain, Seq(0.0, 0.05)).\n",
    "    build()\n",
    "\n",
    "\n",
    "val validator = new TrainValidationSplit().\n",
    "    setSeed(Random.nextLong()).\n",
    "    setEstimator(pipeline).\n",
    "    setEvaluator(evaluator).\n",
    "    setEstimatorParamMaps(paramGrid).\n",
    "    setTrainRatio(0.9)\n",
    "\n",
    "val validatorModel = validator.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res22: org.apache.spark.ml.param.ParamMap =\n",
      "{\n",
      "\tdtc_624f0acc2372-cacheNodeIds: false,\n",
      "\tdtc_624f0acc2372-checkpointInterval: 10,\n",
      "\tdtc_624f0acc2372-featuresCol: featureVector,\n",
      "\tdtc_624f0acc2372-impurity: entropy,\n",
      "\tdtc_624f0acc2372-labelCol: Cover_Type,\n",
      "\tdtc_624f0acc2372-maxBins: 300,\n",
      "\tdtc_624f0acc2372-maxDepth: 20,\n",
      "\tdtc_624f0acc2372-maxMemoryInMB: 256,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.0,\n",
      "\tdtc_624f0acc2372-minInstancesPerNode: 1,\n",
      "\tdtc_624f0acc2372-predictionCol: prediction,\n",
      "\tdtc_624f0acc2372-probabilityCol: probability,\n",
      "\tdtc_624f0acc2372-rawPredictionCol: rawPrediction,\n",
      "\tdtc_624f0acc2372-seed: 6205718850843740355\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "validatorModel.bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap   // BEST 모델의 parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9092202125819286\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: entropy,\n",
      "\tdtc_624f0acc2372-maxBins: 300,\n",
      "\tdtc_624f0acc2372-maxDepth: 20,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.0\n",
      "}\n",
      "\n",
      "0.9080285236511811\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: entropy,\n",
      "\tdtc_624f0acc2372-maxBins: 40,\n",
      "\tdtc_624f0acc2372-maxDepth: 20,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.0\n",
      "}\n",
      "\n",
      "0.90545293789763\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: gini,\n",
      "\tdtc_624f0acc2372-maxBins: 300,\n",
      "\tdtc_624f0acc2372-maxDepth: 20,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.0\n",
      "}\n",
      "\n",
      "0.9045495608049666\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: gini,\n",
      "\tdtc_624f0acc2372-maxBins: 40,\n",
      "\tdtc_624f0acc2372-maxDepth: 20,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.0\n",
      "}\n",
      "\n",
      "0.7271224556480289\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: entropy,\n",
      "\tdtc_624f0acc2372-maxBins: 40,\n",
      "\tdtc_624f0acc2372-maxDepth: 20,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.05\n",
      "}\n",
      "\n",
      "0.7254118054087301\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: entropy,\n",
      "\tdtc_624f0acc2372-maxBins: 300,\n",
      "\tdtc_624f0acc2372-maxDepth: 20,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.05\n",
      "}\n",
      "\n",
      "0.6709016472216349\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: gini,\n",
      "\tdtc_624f0acc2372-maxBins: 300,\n",
      "\tdtc_624f0acc2372-maxDepth: 20,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.05\n",
      "}\n",
      "\n",
      "0.6691525554039249\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: gini,\n",
      "\tdtc_624f0acc2372-maxBins: 40,\n",
      "\tdtc_624f0acc2372-maxDepth: 20,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.05\n",
      "}\n",
      "\n",
      "0.6356314990293501\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: gini,\n",
      "\tdtc_624f0acc2372-maxBins: 300,\n",
      "\tdtc_624f0acc2372-maxDepth: 1,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.05\n",
      "}\n",
      "\n",
      "0.6356314990293501\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: gini,\n",
      "\tdtc_624f0acc2372-maxBins: 300,\n",
      "\tdtc_624f0acc2372-maxDepth: 1,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.0\n",
      "}\n",
      "\n",
      "0.6343437061525746\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: gini,\n",
      "\tdtc_624f0acc2372-maxBins: 40,\n",
      "\tdtc_624f0acc2372-maxDepth: 1,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.05\n",
      "}\n",
      "\n",
      "0.6343437061525746\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: gini,\n",
      "\tdtc_624f0acc2372-maxBins: 40,\n",
      "\tdtc_624f0acc2372-maxDepth: 1,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.0\n",
      "}\n",
      "\n",
      "0.48966882580198745\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: entropy,\n",
      "\tdtc_624f0acc2372-maxBins: 300,\n",
      "\tdtc_624f0acc2372-maxDepth: 1,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.05\n",
      "}\n",
      "\n",
      "0.48966882580198745\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: entropy,\n",
      "\tdtc_624f0acc2372-maxBins: 300,\n",
      "\tdtc_624f0acc2372-maxDepth: 1,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.0\n",
      "}\n",
      "\n",
      "0.48966882580198745\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: entropy,\n",
      "\tdtc_624f0acc2372-maxBins: 40,\n",
      "\tdtc_624f0acc2372-maxDepth: 1,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.05\n",
      "}\n",
      "\n",
      "0.48966882580198745\n",
      "{\n",
      "\tdtc_624f0acc2372-impurity: entropy,\n",
      "\tdtc_624f0acc2372-maxBins: 40,\n",
      "\tdtc_624f0acc2372-maxDepth: 1,\n",
      "\tdtc_624f0acc2372-minInfoGain: 0.0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// 각 조합별 파라미터 \n",
    "validatorModel.validationMetrics.zip(validatorModel.getEstimatorParamMaps).sortBy(_._1).reverse.foreach {\n",
    "    case (metric, paramMap) =>\n",
    "    println(metric)\n",
    "    println(paramMap)\n",
    "    println()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 범주형 변수를 one-hot에서 numerical encoding을 변경 / Tree에 알려주기 /HPO 다시 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unencodeOneHot: (data: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
      "unencodedTrainData: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 11 more fields]\n",
      "inputCols: Array[String] = Array(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, wilderness, soil)\n",
      "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_2bed32fa9733\n",
      "indexer: org.apache.spark.ml.feature.VectorIndexer = vecIdx_50aeb0916000\n",
      "indexerModel: org.apache.spark.ml.feature.VectorIndexerModel = vecIdx_50aeb0916000\n",
      "categoricalFeatures: scala.collection.immutable.Set[Int] = Set(10, 11)\n",
      "Chose 2 categorical features: 10, 11\n",
      "classifier: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_82f67bc881a1\n",
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_7537e0d648e8\n",
      "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
      "Array({\n",
      "\tdtc_82f67bc881a1-impurity: gini,\n",
      "\tdtc_82f67bc881a1-maxBins: 40,\n",
      "\tdtc_82f67bc881a1-maxDepth: 1,\n",
      "\tdtc_82f67bc881a1-minInfoGain: 0.0\n",
      "}, {\n",
      "\tdtc_82f67bc881a1-impurity: gini,\n",
      "\tdtc_82f67bc881a1-maxBins: 300,\n",
      "\tdtc_82f67bc881a1-maxDepth: 1,\n",
      "\tdtc_82f67bc881a1-minInfoGain: 0.0\n",
      "}, {\n",
      "\tdtc_82f67bc881a1-impurity: entropy,\n",
      "\tdtc_82f67bc881a1-maxBins: 40,\n",
      "\tdtc_82f67bc881a1-maxDepth: 1,\n",
      "\tdtc_82f67bc881a1-minInfoGain: 0.0\n",
      "}, {\n",
      "\tdtc_82f67bc881a1-impurity: entropy,\n",
      "\tdtc_82f67bc881a1-maxBins: 300,\n",
      "\tdtc_82f67bc881a1-maxDepth: 1,\n",
      "\tdtc_82f67bc881a1-minInfoGain: 0.0\n",
      "}, {\n",
      "\tdtc_82f67bc881a1-impurity: gini,\n",
      "\tdtc_82f67bc881a1-maxBins: 40,\n",
      "\tdtc_82f67bc881a1-maxDepth: 20,\n",
      "\tdtc_82f67bc881a1-minInfoGain: 0.0\n",
      "}, {\n",
      "\tdtc_82f67bc881a1-impurity: gini,\n",
      "\tdtc_82f67b...multiclassEval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_3bb9b29b9106\n",
      "validator: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_4f2724bfb958\n",
      "validatorModel: org.apache.spark.ml.tuning.TrainValidationSplitModel = tvs_4f2724bfb958\n",
      "bestModel: org.apache.spark.ml.Model[_] = pipeline_7537e0d648e8\n",
      "{\n",
      "\tdtc_82f67bc881a1-cacheNodeIds: false,\n",
      "\tdtc_82f67bc881a1-checkpointInterval: 10,\n",
      "\tdtc_82f67bc881a1-featuresCol: indexedVector,\n",
      "\tdtc_82f67bc881a1-impurity: entropy,\n",
      "\tdtc_82f67bc881a1-labelCol: Cover_Type,\n",
      "\tdtc_82f67bc881a1-maxBins: 300,\n",
      "\tdtc_82f67bc881a1-maxDepth: 20,\n",
      "\tdtc_82f67bc881a1-maxMemoryInMB: 256,\n",
      "\tdtc_82f67bc881a1-minInfoGain: 0.0,\n",
      "\tdtc_82f67bc881a1-minInstancesPerNode: 1,\n",
      "\tdtc_82f67bc881a1-predictionCol: prediction,\n",
      "\tdtc_82f67bc881a1-probabilityCol: probability,\n",
      "\tdtc_82f67bc881a1-rawPredictionCol: rawPrediction,\n",
      "\tdtc_82f67bc881a1-seed: -6385230447705030233\n",
      "}\n",
      "testAccuracy: Double = 0.9676738920679806\n",
      "0.9676738920679806\n"
     ]
    }
   ],
   "source": [
    "// 데이터 변환 -> 분류 -> 평가 \n",
    "\n",
    "def unencodeOneHot(data:DataFrame): DataFrame = {\n",
    "    \n",
    "    val wildernessCols = (0 until 4).map( i => s\"Wilderness_Area_$i\" ).toArray\n",
    "        \n",
    "    val wildernessAssembler = new VectorAssembler().\n",
    "        setInputCols(wildernessCols).\n",
    "        setOutputCol(\"wilderness\")\n",
    "    \n",
    "    val unhotUDF = udf { (vec:Vector) => vec.toArray.indexOf(1) }\n",
    "    \n",
    "    val withWilderness = wildernessAssembler.transform(data).\n",
    "        drop(wildernessCols: _*).\n",
    "        withColumn(\"wilderness\", unhotUDF($\"wilderness\"))\n",
    "    \n",
    "    val soilCols = (0 until 40).map( i => s\"Soil_Type_$i\" ).toArray\n",
    "    \n",
    "    val soilAssembler = new VectorAssembler().\n",
    "        setInputCols(soilCols).\n",
    "        setOutputCol(\"soil\")\n",
    "    \n",
    "    soilAssembler.transform(withWilderness).\n",
    "        drop(soilCols: _*).\n",
    "        withColumn(\"soil\", unhotUDF($\"soil\"))\n",
    "}\n",
    "\n",
    "val unencTrainData = unencodeOneHot(trainData)\n",
    "val unencTestData = unencodeOneHot(testData) \n",
    "\n",
    "val inputCols = unencTrainData.columns.filterNot( _ == \"Cover_Type\")\n",
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(inputCols).\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "val indexer = new VectorIndexer().\n",
    "    setInputCol(\"featureVector\").\n",
    "    setOutputCol(\"indexedVector\").\n",
    "    setMaxCategories(40)\n",
    "\n",
    "// FOR DEBUG\n",
    "// val indexerModel = indexer.fit(assembler.transform(unencTrainData))\n",
    "// val categoricalFeatures = indexerModel.categoryMaps.keys.toSet\n",
    "// println(s\"Chose ${categoricalFeatures.size} \" +\n",
    "//         s\"categorical features: ${categoricalFeatures.mkString(\", \")}\")\n",
    "\n",
    "\n",
    "val classifier = new DecisionTreeClassifier().\n",
    "    setSeed(Random.nextLong()).\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setFeaturesCol(\"indexedVector\").\n",
    "    setPredictionCol(\"prediction\")\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, indexer, classifier))\n",
    "\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "    addGrid(classifier.impurity, Seq(\"gini\", \"entropy\")).\n",
    "    addGrid(classifier.maxDepth, Seq(1, 20)).\n",
    "    addGrid(classifier.maxBins, Seq(40, 300)).\n",
    "    addGrid(classifier.minInfoGain, Seq(0.0, 0.05)).\n",
    "    build()\n",
    "\n",
    "val multiclassEval = new MulticlassClassificationEvaluator().\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setPredictionCol(\"prediction\").\n",
    "    setMetricName(\"accuracy\")\n",
    "\n",
    "val validator = new TrainValidationSplit().\n",
    "    setSeed(Random.nextLong()).\n",
    "    setEstimator(pipeline).\n",
    "    setEvaluator(multiclassEval).\n",
    "    setEstimatorParamMaps(paramGrid).\n",
    "    setTrainRatio(0.9)\n",
    "\n",
    "val validatorModel = validator.fit(unencTrainData)\n",
    "\n",
    "val bestModel = validatorModel.bestModel\n",
    "\n",
    "println(bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap)\n",
    "\n",
    "val testAccuracy = multiclassEval.evaluate(bestModel.transform(unencTestData))\n",
    "println(testAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 랜덤 포레스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bff5da430384636b2f81425d97c53e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "\n",
    "\n",
    "def evaluateForest() = {\n",
    "    val unencTrainData = unencodeOneHot(trainData)\n",
    "    val unencTestData = unencodeOneHot(testData)    \n",
    "\n",
    "    val inputCols = unencTrainData.columns.filterNot( _ == \"Cover_Type\")\n",
    "    val assembler = new VectorAssembler().\n",
    "        setInputCols(inputCols).\n",
    "        setOutputCol(\"featureVector\")\n",
    "\n",
    "    val indexer = new VectorIndexer().\n",
    "        setInputCol(\"featureVector\").\n",
    "        setOutputCol(\"indexedVector\").\n",
    "        setMaxCategories(40)\n",
    "\n",
    "    val classifier = new RandomForestClassifier().\n",
    "        setSeed(Random.nextLong()).\n",
    "        setLabelCol(\"Cover_Type\").\n",
    "        setFeaturesCol(\"indexedVector\").\n",
    "        setPredictionCol(\"prediction\")    \n",
    "\n",
    "    val pipeline = new Pipeline().setStages(Array(assembler, indexer, classifier))\n",
    "    \n",
    "    val paramGrid = new ParamGridBuilder().\n",
    "        addGrid(classifier.impurity, Seq(\"entropy\")).\n",
    "        addGrid(classifier.maxDepth, Seq(20)).\n",
    "        addGrid(classifier.maxBins, Seq(300)).\n",
    "        addGrid(classifier.minInfoGain, Seq(0.0)).\n",
    "        build()\n",
    "\n",
    "    val multiclassEval = new MulticlassClassificationEvaluator().\n",
    "        setLabelCol(\"Cover_Type\").\n",
    "        setPredictionCol(\"prediction\").\n",
    "        setMetricName(\"accuracy\")\n",
    "\n",
    "    val validator = new TrainValidationSplit().\n",
    "        setSeed(Random.nextLong()).\n",
    "        setEstimator(pipeline).\n",
    "        setEvaluator(multiclassEval).\n",
    "        setEstimatorParamMaps(paramGrid).\n",
    "        setTrainRatio(0.9)\n",
    "\n",
    "    val validatorModel = validator.fit(unencTrainData)\n",
    "\n",
    "    val bestModel = validatorModel.bestModel\n",
    "\n",
    "    println(bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap)\n",
    "\n",
    "    val testAccuracy = multiclassEval.evaluate(bestModel.transform(unencTestData))\n",
    "    println(testAccuracy)    \n",
    "}\n",
    "\n",
    "evaluateForest()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
