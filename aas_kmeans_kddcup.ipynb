{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 스파크 고급 분석 5장\n",
    "\n",
    "## K-평균 군집화로 네트워크 이상 탐지하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'proxyUser': 'hduser', 'driverMemory': '4000M', 'conf': {'spark.jars.packages': 'graphframes:graphframes:0.3.0-spark2.0-s_2.11', 'spark.master': 'local[2]', 'spark.sql.crossJoin.enabled': 'true'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"proxyUser\": \"hduser\",\n",
    "    \"driverMemory\": \"4000M\", \n",
    "    \"conf\": {\"spark.jars.packages\": \"graphframes:graphframes:0.3.0-spark2.0-s_2.11\",\n",
    "             \"spark.master\": \"local[2]\",\n",
    "             \"spark.sql.crossJoin.enabled\": \"true\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.Pipeline\n",
      "import org.apache.spark.ml.clustering.{KMeans, KMeansModel}\n",
      "import org.apache.spark.ml.feature.VectorAssembler\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.spark.ml.feature.StandardScaler\n",
      "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base: String = hdfs://localhost:54310/kddcup/\n"
     ]
    }
   ],
   "source": [
    "val base = \"hdfs://localhost:54310/kddcup/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습/평가 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataWithoutHeader: org.apache.spark.sql.DataFrame = [_c0: int, _c1: string ... 40 more fields]\n",
      "colNames: Seq[String] = List(duration, protocol_type, service, flag, src_bytes, dst_bytes, land, wrong_fragment, urgent, hot, num_failed_logins, logged_in, num_compromised, root_shell, su_attempted, num_root, num_file_creations, num_shells, num_access_files, num_outbound_cmds, is_host_login, is_guest_login, count, srv_count, serror_rate, srv_serror_rate, rerror_rate, srv_rerror_rate, same_srv_rate, diff_srv_rate, srv_diff_host_rate, dst_host_count, dst_host_srv_count, dst_host_same_srv_rate, dst_host_diff_srv_rate, dst_host_same_src_port_rate, dst_host_srv_diff_host_rate, dst_host_serror_rate, dst_host_srv_serror_rate, dst_host_rerror_rate, dst_host_srv_rerror_rate, label)\n",
      "data: org.apache.spark.sql.DataFrame = [duration: int, protocol_type: string ... 40 more fields]\n"
     ]
    }
   ],
   "source": [
    "val dataWithoutHeader = spark.read.option(\"inferSchema\", \"true\").option(\"header\", false).csv(base + \"kddcup.data_10_percent\")\n",
    "\n",
    "val colNames = Seq(\n",
    "    \"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    "    \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "    \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "    \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "    \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    "    \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "    \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "    \"dst_host_count\", \"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "    \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "    \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    "    \"label\")\n",
    "\n",
    "val data = dataWithoutHeader.toDF(colNames: _*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res3: org.apache.spark.sql.Row = [0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.0,0.0,0.0,0.0,1.0,0.0,0.0,9,9,1.0,0.0,0.11,0.0,0.0,0.0,0.0,0.0,normal.]\n"
     ]
    }
   ],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeas 로 군집 형성 예시 (k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numericOnly: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [duration: int, src_bytes: int ... 37 more fields]\n",
      "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_8275d28d54b2\n",
      "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_0cc7d4bb2def\n",
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_758873ba8a0b\n",
      "pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_758873ba8a0b\n",
      "kmeansModel: org.apache.spark.ml.clustering.KMeansModel = kmeans_0cc7d4bb2def\n",
      "[47.979395571029514,1622.078830816566,868.5341828266062,4.453261001578883E-5,0.006432937937735314,1.4169466823205539E-5,0.03451682118132869,1.5181571596291647E-4,0.14824703453301485,0.01021213716043885,1.1133152503947209E-4,3.6435771831099954E-5,0.011351767134933808,0.0010829521072021374,1.0930731549329986E-4,0.0010080563539937655,0.0,0.0,0.0013865835391279706,332.2862475203433,292.9071434354884,0.17668541759442902,0.17660780940042928,0.05743309987449906,0.05771839196793657,0.7915488441768395,0.020981640419422482,0.028996862475204364,232.4707319541719,188.6660459090725,0.7537812031907427,0.030905611108822645,0.6019355289260995,0.006683514837456327,0.1767539573296518,0.17644162179667539,0.05811762681672781,0.05741111695882666]\n",
      "[2.0,6.9337564E8,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,57.0,3.0,0.79,0.67,0.21,0.33,0.05,0.39,0.0,255.0,3.0,0.01,0.09,0.22,0.0,0.18,0.67,0.05,0.33]\n"
     ]
    }
   ],
   "source": [
    "val numericOnly = data.drop(\"protocol_type\", \"service\", \"flag\").cache()   // 일단 string 타입은 drop\n",
    "\n",
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(numericOnly.columns.filter( _ != \"label\")).\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "val kmeans = new KMeans().\n",
    "//     setSeed(1L).\n",
    "//     setK(2).\n",
    "    setFeaturesCol(\"featureVector\").\n",
    "    setPredictionCol(\"cluster\") \n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, kmeans))\n",
    "\n",
    "val pipelineModel = pipeline.fit(numericOnly)\n",
    "val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\n",
    "\n",
    "kmeansModel.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 군집에 들어가는 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "withCluster: org.apache.spark.sql.DataFrame = [duration: int, src_bytes: int ... 39 more fields]\n",
      "+-------+----------------+------+\n",
      "|cluster|label           |count |\n",
      "+-------+----------------+------+\n",
      "|0      |smurf.          |280790|\n",
      "|0      |neptune.        |107201|\n",
      "|0      |normal.         |97278 |\n",
      "|0      |back.           |2203  |\n",
      "|0      |satan.          |1589  |\n",
      "|0      |ipsweep.        |1247  |\n",
      "|0      |portsweep.      |1039  |\n",
      "|0      |warezclient.    |1020  |\n",
      "|0      |teardrop.       |979   |\n",
      "|0      |pod.            |264   |\n",
      "|0      |nmap.           |231   |\n",
      "|0      |guess_passwd.   |53    |\n",
      "|0      |buffer_overflow.|30    |\n",
      "|0      |land.           |21    |\n",
      "|0      |warezmaster.    |20    |\n",
      "|0      |imap.           |12    |\n",
      "|0      |rootkit.        |10    |\n",
      "|0      |loadmodule.     |9     |\n",
      "|0      |ftp_write.      |8     |\n",
      "|0      |multihop.       |7     |\n",
      "+-------+----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val withCluster = pipelineModel.transform(numericOnly)\n",
    "\n",
    "withCluster.\n",
    "    groupBy(\"cluster\", \"label\").count.\n",
    "    orderBy($\"cluster\", $\"count\".desc).show(false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K 선정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusterScore: (df: org.apache.spark.sql.DataFrame, k: Int)Double\n",
      "(20,3.4534470232165543E13)\n",
      "(40,1.6863401552513182E13)\n",
      "(60,1.592796307162011E13)\n",
      "(80,1.3780873599313395E13)\n",
      "(100,1.2477951244097455E13)\n"
     ]
    }
   ],
   "source": [
    "def clusterScore(df:DataFrame, k:Int) = {\n",
    "    val assembler = new VectorAssembler().\n",
    "        setInputCols(numericOnly.columns.filter( _ != \"label\")).\n",
    "        setOutputCol(\"featureVector\")\n",
    "\n",
    "    val kmeans = new KMeans().\n",
    "        setSeed(1L).\n",
    "        setK(k).\n",
    "        setMaxIter(40).  // default : 20\n",
    "        setTol(1.0e-5). \n",
    "        setFeaturesCol(\"featureVector\").\n",
    "        setPredictionCol(\"cluster\") \n",
    "\n",
    "    val pipeline = new Pipeline().setStages(Array(assembler, kmeans))\n",
    "\n",
    "    val pipelineModel = pipeline.fit(df)\n",
    "    val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\n",
    "    \n",
    "    kmeansModel.summary.trainingCost\n",
    "    //kmeansModel.computeCost(assembler.transform(df)) / df.count\n",
    "}\n",
    "\n",
    "(20 to 100 by 20).map( k => (k, clusterScore(numericOnly, k))).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특징 정규화 후 최적  k 다시 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.StandardScaler\n",
      "clusterScoreWithNormalized: (df: org.apache.spark.sql.DataFrame, k: Int)Double\n",
      "(20,3732362.744115213)\n",
      "(40,1024412.0304890894)\n",
      "(60,538699.248454132)\n",
      "(80,385748.6877572641)\n",
      "(100,269503.7740490003)\n"
     ]
    }
   ],
   "source": [
    "def clusterScoreWithNormalized(df:DataFrame, k:Int) = {\n",
    "    val assembler = new VectorAssembler().\n",
    "        setInputCols(numericOnly.columns.filter( _ != \"label\")).\n",
    "        setOutputCol(\"featureVector\")\n",
    "    \n",
    "    val scaler = new StandardScaler()\n",
    "      .setInputCol(\"featureVector\")\n",
    "      .setOutputCol(\"scaledFeatureVector\")\n",
    "      .setWithStd(true)\n",
    "      .setWithMean(false)    // KMEANS는 translation 해도 결과에 무관\n",
    "     \n",
    "    val kmeans = new KMeans().\n",
    "        setSeed(1L).\n",
    "        setK(k).\n",
    "        setMaxIter(40).  // default : 20\n",
    "        setTol(1.0e-5). \n",
    "        setFeaturesCol(\"scaledFeatureVector\").\n",
    "        setPredictionCol(\"cluster\") \n",
    "\n",
    "    val pipeline = new Pipeline().setStages(Array(assembler, scaler, kmeans))\n",
    "    val pipelineModel = pipeline.fit(df)\n",
    "    val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\n",
    "    \n",
    "    kmeansModel.summary.trainingCost\n",
    "    //kmeansModel.computeCost(assembler.transform(df)) / df.count\n",
    "}\n",
    "\n",
    "(20 to 100 by 20).map( k => (k, clusterScoreWithNormalized(numericOnly, k))).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 범주형 변수 살리기  (StringIndexer, OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "oneHotPipeline: (inputCol: String)(org.apache.spark.ml.Pipeline, String)\n",
      "clusterScoreWithOneHot: (df: org.apache.spark.sql.DataFrame, k: Int)Double\n",
      "res147: data.type = [duration: int, protocol_type: string ... 40 more fields]\n",
      "(200,644140.150984261)\n",
      "(220,546771.4262406854)\n",
      "(240,468787.3598376977)\n",
      "(260,420861.01543326344)\n",
      "res151: data.type = [duration: int, protocol_type: string ... 40 more fields]\n"
     ]
    }
   ],
   "source": [
    "def oneHotPipeline(inputCol:String): (Pipeline, String) = {\n",
    "    \n",
    "    val indexer = new StringIndexer().\n",
    "        setInputCol(inputCol).\n",
    "        setOutputCol(inputCol + \"_indexed\")\n",
    "    \n",
    "    val encoder = new OneHotEncoder().\n",
    "        setInputCol(inputCol + \"_indexed\").\n",
    "        setOutputCol(inputCol + \"_vec\")\n",
    "    \n",
    "    val pipeline = new Pipeline().setStages(Array(indexer, encoder))\n",
    "    \n",
    "    (pipeline, inputCol + \"_vec\")   \n",
    "}\n",
    "\n",
    "def clusterScoreWithOneHot(df:DataFrame, k:Int) = {\n",
    "    \n",
    "    val (protoTypeEncoder, protoTypeVecCol) = oneHotPipeline(\"protocol_type\")\n",
    "    val (serviceEncoder, serviceVecCol) = oneHotPipeline(\"service\")\n",
    "    val (flagEncoder, flagVecCol) = oneHotPipeline(\"flag\")\n",
    "    \n",
    "    val inputCols = numericOnly.columns.filter( _ != \"label\") ++ Seq(protoTypeVecCol, serviceVecCol, flagVecCol)\n",
    "    val assembler = new VectorAssembler().\n",
    "        setInputCols(inputCols).\n",
    "        setOutputCol(\"featureVector\")\n",
    "    \n",
    "    val scaler = new StandardScaler()\n",
    "      .setInputCol(\"featureVector\")\n",
    "      .setOutputCol(\"scaledFeatureVector\")\n",
    "      .setWithStd(true)\n",
    "      .setWithMean(false)    // KMEANS는 translation 해도 결과에 무관\n",
    "     \n",
    "    val kmeans = new KMeans().\n",
    "        setSeed(1L).\n",
    "        setK(k).\n",
    "        setMaxIter(40).  // default : 20\n",
    "        setTol(1.0e-5). \n",
    "        setFeaturesCol(\"scaledFeatureVector\").\n",
    "        setPredictionCol(\"cluster\") \n",
    "\n",
    "    val pipeline = new Pipeline().setStages(Array(protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans))\n",
    "    val pipelineModel = pipeline.fit(df)\n",
    "    val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\n",
    "    \n",
    "    kmeansModel.summary.trainingCost\n",
    "    //kmeansModel.computeCost(assembler.transform(df)) / df.count\n",
    "}\n",
    "\n",
    "\n",
    "data.cache()\n",
    "\n",
    "(200 to 260 by 20).map( k => (k, clusterScoreWithOneHot(data, k))).foreach(println)\n",
    "\n",
    "data.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getBestModel: (df: org.apache.spark.sql.DataFrame, k: Int)org.apache.spark.ml.PipelineModel\n",
      "res225: data.type = [duration: int, protocol_type: string ... 40 more fields]\n",
      "pipeline: org.apache.spark.ml.PipelineModel = pipeline_39293a8df720\n",
      "kMeansModel: org.apache.spark.ml.clustering.KMeansModel = kmeans_3bfe15c0fed5\n",
      "centroids: Array[org.apache.spark.ml.linalg.Vector] = Array([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10060435343008005,0.02637193415721355,2.617728048009747,2.6237795820928587,0.014620906010109434,0.0012770582696820874,0.9136059317621815,2.7458512969965354,0.004324061900025904,3.9384344879990563,0.08516361323944795,0.08607793681410496,0.7409707902151725,0.010423867296165644,0.0,2.6093223610926355,2.625226947857916,0.006329520858093144,0.0,0.0,2.0553630063998165,0.0,2.3147536376709317,0.1016688615847554,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0...predictions: org.apache.spark.sql.DataFrame = [duration: int, protocol_type: string ... 49 more fields]\n",
      "threshold: Double = 359.5635086630823\n",
      "res231: data.type = [duration: int, protocol_type: string ... 40 more fields]\n"
     ]
    }
   ],
   "source": [
    "def getBestModel(df:DataFrame, k:Int) = {\n",
    "    val (protoTypeEncoder, protoTypeVecCol) = oneHotPipeline(\"protocol_type\")\n",
    "    val (serviceEncoder, serviceVecCol) = oneHotPipeline(\"service\")\n",
    "    val (flagEncoder, flagVecCol) = oneHotPipeline(\"flag\")\n",
    "\n",
    "    val inputCols = numericOnly.columns.filter( _ != \"label\") ++ Seq(protoTypeVecCol, serviceVecCol, flagVecCol)\n",
    "    val assembler = new VectorAssembler().\n",
    "        setInputCols(inputCols).\n",
    "        setOutputCol(\"featureVector\")\n",
    "\n",
    "    val scaler = new StandardScaler()\n",
    "      .setInputCol(\"featureVector\")\n",
    "      .setOutputCol(\"scaledFeatureVector\")\n",
    "      .setWithStd(true)\n",
    "      .setWithMean(false)    // KMEANS는 translation 해도 결과에 무관\n",
    "\n",
    "    val kmeans = new KMeans().\n",
    "        setSeed(1L).\n",
    "        setK(k).\n",
    "        setMaxIter(40).  // default : 20\n",
    "        setTol(1.0e-5). \n",
    "        setFeaturesCol(\"scaledFeatureVector\").\n",
    "        setPredictionCol(\"cluster\") \n",
    "\n",
    "    val pipeline = new Pipeline().setStages(Array(protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans))\n",
    "    pipeline.fit(df)\n",
    "}\n",
    "\n",
    "data.cache()\n",
    "\n",
    "val pipeline = getBestModel(data, 260)\n",
    "\n",
    "val kMeansModel = pipeline.stages.last.asInstanceOf[KMeansModel]\n",
    "val centroids = kMeansModel.clusterCenters\n",
    "val predictions = pipeline.transform(data)\n",
    "\n",
    "// centroid사의의 거리를 구하고 100 번째 거리를 threshold로 정한다.\n",
    "val threshold = predictions.select(\"cluster\", \"scaledFeatureVector\").as[(Int, Vector)].\n",
    "    map { case (cluster, vec) => \n",
    "        Vectors.sqdist(centroids(cluster), vec)\n",
    "    }.\n",
    "    orderBy($\"value\".desc).take(100).last\n",
    "\n",
    "data.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anomaliy 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomalies: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [duration: int, protocol_type: string ... 49 more fields]\n",
      "res262: Long = 100\n"
     ]
    }
   ],
   "source": [
    "val anomalies = predictions.filter { row =>\n",
    "    val vec = row.getAs[Vector](\"scaledFeatureVector\")\n",
    "    val cluster = row.getAs[Int](\"cluster\")\n",
    "    \n",
    "    val dist = Vectors.sqdist(centroids(cluster), vec)\n",
    "    \n",
    "    dist >= threshold\n",
    "}\n",
    "\n",
    "anomalies.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res263: org.apache.spark.sql.Row = [257,tcp,telnet,SF,181,1222,0,0,0,0,0,1,0,0,0,0,2,0,0,0,0,0,1,1,0.0,0.0,0.0,0.0,1.0,0.0,0.0,62,15,0.21,0.05,0.02,0.13,0.03,0.13,0.0,0.0,normal.,1.0,(2,[1],[1.0]),11.0,(65,[11],[1.0]),0.0,(10,[0],[1.0]),(115,[0,1,2,8,13,19,20,25,28,29,30,31,32,33,34,35,39,51,105],[257.0,181.0,1222.0,1.0,2.0,1.0,1.0,1.0,62.0,15.0,0.21,0.05,0.02,0.13,0.03,0.13,1.0,1.0,1.0]),(115,[0,1,2,8,13,19,20,25,28,29,30,31,32,33,34,35,39,51,105],[0.3631243814792357,1.83157948440338E-4,0.03698547075334117,2.814168444874875,20.74347117165251,0.0046915887460480836,0.004059713227290898,2.5760614800997756,0.9575972778839426,0.14145547123294683,0.5112213357966311,0.45762773452021277,0.04155332553971481,3.0854766450748023,0.0788243407356959,0.3412795032215277,2.055363006399789,31.0483974914..."
     ]
    }
   ],
   "source": [
    "anomalies.first"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
